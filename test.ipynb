{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/anhnh/miniconda3/envs/commonenv/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "# t_tokenizer = AutoTokenizer.from_pretrained(\"nvidia/NV-Embed-v2\")\n",
    "t_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"McGill-NLP/LLM2Vec-Meta-Llama-3-8B-Instruct-mntp\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t_tokenizer.add_eos_token = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, os\n",
    "\n",
    "def load(path):\n",
    "    with open(path) as f:\n",
    "        data = [json.loads(line) for line in f]\n",
    "    return data\n",
    "\n",
    "dataset_names = [\"MAVEN\", \"ACE\"]\n",
    "perm_ids = [0, 1, 2, 3, 4]\n",
    "\n",
    "root=\"./data_incremental_2\"\n",
    "\n",
    "# for name in dataset_names:\n",
    "#     root_name = root + \"/\" + name + \"/\"\n",
    "#     test_path = root_name + f\"{name}.test.jsonl\"\n",
    "#     for \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAVEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_trigger(tokens, span):\n",
    "    trigger = tokenizer.decode(tokens[span[0]:span[1] + 1])\n",
    "    step = span[1] - span[0] + 1\n",
    "    order = 0\n",
    "    for i in range(len(tokens) - step + 1):\n",
    "        if tokenizer.decode(tokens[i:i+step]) == trigger:\n",
    "            if i == span[0]:\n",
    "                break\n",
    "            order += 1\n",
    "    return trigger, order\n",
    "\n",
    "def get_span(tokenized, trigger, order, prefix_length=16):\n",
    "    step = len(t_tokenizer.tokenize(trigger))\n",
    "    n_order = 0\n",
    "    span = None\n",
    "    for i in range(len(tokenized) - step + 1):\n",
    "        if t_tokenizer.decoder.decode(tokenized[i:i+step]) == trigger:\n",
    "            if n_order == order:\n",
    "                span = [i+prefix_length, i+step-1+prefix_length]\n",
    "            n_order += 1\n",
    "\n",
    "    if not span:\n",
    "        step += 1\n",
    "        n_order = 0\n",
    "        for i in range(len(tokenized) - step + 1):\n",
    "            if t_tokenizer.decoder.decode(tokenized[i:i+step]) == trigger:\n",
    "                if n_order == order:\n",
    "                    span = [i+prefix_length, i+step-1+prefix_length]\n",
    "                n_order += 1\n",
    "    \n",
    "    if not span:\n",
    "        step += 1\n",
    "        n_order = 0\n",
    "        for i in range(len(tokenized) - step + 1):\n",
    "            if t_tokenizer.decoder.decode(tokenized[i:i+step]) == trigger:\n",
    "                if n_order == order:\n",
    "                    span = [i+prefix_length, i+step-1+prefix_length]\n",
    "                n_order += 1\n",
    "    \n",
    "    if not span:\n",
    "        step -= 2\n",
    "        order -= 1\n",
    "        n_order = 0\n",
    "        for i in range(len(tokenized) - step + 1):\n",
    "            if t_tokenizer.decoder.decode(tokenized[i:i+step]) == trigger:\n",
    "                if n_order == order:\n",
    "                    span = [i+prefix_length, i+step-1+prefix_length]\n",
    "                n_order += 1\n",
    "\n",
    "    return span\n",
    "\n",
    "def format_trigger(trigger):\n",
    "    if trigger == \"' s\": return \"'s\"\n",
    "    if trigger == \"' re\": return \"'re\"\n",
    "    if trigger == \"' ve\": return \"'ve\"\n",
    "    if trigger == \"' ll\": return \"'ll\"\n",
    "    if trigger == \"' d\": return \"'d\"\n",
    "    if trigger[0] == \"'\": return trigger[1:].strip()\n",
    "    if trigger[0] == \".\":\n",
    "        trigger = trigger[1:].strip()\n",
    "    try:\n",
    "        if trigger[-1] == \".\":\n",
    "            trigger = trigger[:-1].strip()\n",
    "    except:\n",
    "        pass \n",
    "    return trigger.replace(\"n't\", \" not\").strip()\n",
    "    # if trigger[0]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load(\"./data_incremental_2/MAVEN/MAVEN.test.jsonl\")\n",
    "\n",
    "prefix = t_tokenizer.encode(\"Classify the type of event corresponding to each token in the following sentence:\")[:-1]\n",
    "\n",
    "count = 0\n",
    "\n",
    "with open(\"./data_incremental/MAVEN/MAVEN.test.jsonl\", 'w') as file:\n",
    "    for row in data:\n",
    "        tokens = row['piece_ids']\n",
    "        spans = row['span']\n",
    "        labels = row['label']\n",
    "        text = tokenizer.decode(tokens, skip_special_tokens=True)\n",
    "        text = text.replace(\"n't\", \" not\")\n",
    "        new_ids = prefix + t_tokenizer.encode(text)[1:]\n",
    "        \n",
    "        n_spans = []\n",
    "        n_tokenized = t_tokenizer.tokenize(text)\n",
    "        for i, span in enumerate(spans):\n",
    "            trigger, order = get_trigger(tokens, span)\n",
    "            trigger = format_trigger(trigger)\n",
    "            n_span = get_span(n_tokenized, trigger, order)\n",
    "            if not n_span:\n",
    "                del labels[i]\n",
    "                print(trigger, span, order, text, tokens)\n",
    "                continue\n",
    "                \n",
    "                # if count > 20: break\n",
    "            n_spans.append(n_span)\n",
    "        \n",
    "        # if count > 20: break\n",
    "        \n",
    "        row[\"piece_ids\"] = new_ids\n",
    "        row[\"span\"] = n_spans\n",
    "        row[\"label\"] = labels\n",
    "\n",
    "        json_record = json.dumps(row)\n",
    "        file.write(json_record + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ACE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_trigger(tokens, span):\n",
    "    trigger = tokenizer.decode(tokens[span[0]:span[1] + 1])\n",
    "    step = span[1] - span[0] + 1\n",
    "    order = 0\n",
    "    for i in range(len(tokens) - step + 1):\n",
    "        if tokenizer.decode(tokens[i:i+step]) == trigger:\n",
    "            if i == span[0]:\n",
    "                break\n",
    "            order += 1\n",
    "    return trigger, order\n",
    "\n",
    "def get_span(tokenized, trigger, order):\n",
    "    step = len(t_tokenizer.tokenize(trigger))\n",
    "    n_order = 0\n",
    "    span = None\n",
    "    for i in range(len(tokenized) - step + 1):\n",
    "        if t_tokenizer.decoder.decode(tokenized[i:i+step]).strip() == trigger:\n",
    "            if n_order == order:\n",
    "                span = [i, i+step-1]\n",
    "            n_order += 1\n",
    "    \n",
    "    if (not span) and (step > 1):\n",
    "        step -= 1\n",
    "        n_order = 0\n",
    "        for i in range(len(tokenized) - step + 1):\n",
    "            if t_tokenizer.decoder.decode(tokenized[i:i+step]).strip() == trigger:\n",
    "                if n_order == order:\n",
    "                    span = [i, i+step-1]\n",
    "                n_order += 1\n",
    "        step += 1\n",
    "    \n",
    "    if (not span) and (step > 2):\n",
    "        step -= 2\n",
    "        n_order = 0\n",
    "        for i in range(len(tokenized) - step + 1):\n",
    "            if t_tokenizer.decoder.decode(tokenized[i:i+step]).strip() == trigger:\n",
    "                if n_order == order:\n",
    "                    span = [i, i+step-1]\n",
    "                n_order += 1\n",
    "        step += 2\n",
    "\n",
    "    if not span:\n",
    "        step = len(t_tokenizer.tokenize(trigger)) + 1\n",
    "        n_order = 0\n",
    "        for i in range(len(tokenized) - step + 1):\n",
    "            if t_tokenizer.decoder.decode(tokenized[i:i+step]).strip() == trigger:\n",
    "                if n_order == order:\n",
    "                    span = [i, i+step-1]\n",
    "                n_order += 1\n",
    "    \n",
    "    if not span:\n",
    "        step = len(t_tokenizer.tokenize(trigger)) + 2\n",
    "        n_order = 0\n",
    "        for i in range(len(tokenized) - step + 1):\n",
    "            if t_tokenizer.decoder.decode(tokenized[i:i+step]).strip() == trigger:\n",
    "                if n_order == order:\n",
    "                    span = [i, i+step-1]\n",
    "                n_order += 1\n",
    "    \n",
    "    if not span:\n",
    "        step = len(t_tokenizer.tokenize(trigger))\n",
    "        order -= 1\n",
    "        n_order = 0\n",
    "        for i in range(len(tokenized) - step + 1):\n",
    "            if t_tokenizer.decoder.decode(tokenized[i:i+step]).strip() == trigger:\n",
    "                if n_order == order:\n",
    "                    span = [i, i+step-1]\n",
    "                n_order += 1\n",
    "        order += 1\n",
    "\n",
    "    if not span:\n",
    "        step = 1\n",
    "        n_order = 0\n",
    "        for i in range(len(tokenized) - step + 1):\n",
    "            if t_tokenizer.decoder.decode(tokenized[i:i+step]).strip() == trigger:\n",
    "                if n_order == order:\n",
    "                    span = [i, i+step-1]\n",
    "                n_order += 1\n",
    "    \n",
    "    if not span:\n",
    "        trigger = \"'\" + trigger\n",
    "        n_order = 0\n",
    "        step = len(t_tokenizer.tokenize(trigger))\n",
    "        for i in range(len(tokenized) - step + 1):\n",
    "            if t_tokenizer.decoder.decode(tokenized[i:i+step]).strip() == trigger:\n",
    "                if n_order == order:\n",
    "                    span = [i, i+step-1]\n",
    "                n_order += 1\n",
    "    span[0] += 1\n",
    "    span[1] += 1\n",
    "    return span\n",
    "\n",
    "def format_trigger(trigger):\n",
    "    if trigger == \"' s\": return \"'s\"\n",
    "    if trigger == \"' re\": return \"'re\"\n",
    "    if trigger == \"' ve\": return \"'ve\"\n",
    "    if trigger == \"' ll\": return \"'ll\"\n",
    "    if trigger == \"' d\": return \"'d\"\n",
    "    if trigger[0] == \"'\": return trigger[1:].strip()\n",
    "    if trigger[0] == \".\":\n",
    "        trigger = trigger[1:].strip()\n",
    "    try:\n",
    "        if trigger[-1] == \".\":\n",
    "            trigger = trigger[:-1].strip()\n",
    "    except:\n",
    "        pass \n",
    "    return trigger.replace(\"n't\", \" not\").strip()\n",
    "    # if trigger[0]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_valid(labels, spans):\n",
    "    valid_labels = []\n",
    "    for label in labels:\n",
    "        if label == 0:\n",
    "            break\n",
    "        valid_labels.append(label)\n",
    "    return valid_labels, spans[:len(valid_labels)]\n",
    "\n",
    "def get_ignore(spans):\n",
    "    ignores = []\n",
    "    for span in spans:\n",
    "        ignores += list(range(span[0], span[1] + 1))\n",
    "\n",
    "    return ignores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load(\"./data_incremental_2/ACE/ACE.test.jsonl\")\n",
    "\n",
    "# prefix = t_tokenizer.encode(\"Classify the type of event corresponding to each token in the following sentence:\")\n",
    "# prefix = []\n",
    "\n",
    "eos_token_id = t_tokenizer.eos_token_id\n",
    "count = 0\n",
    "\n",
    "with open(\"./data_incremental/ACE/ACE.test.jsonl\", 'w') as file:\n",
    "    for row in data:\n",
    "        tokens = row['piece_ids']\n",
    "        spans = row['span']\n",
    "        labels = row['label']\n",
    "\n",
    "        labels, spans = get_valid(labels, spans)\n",
    "\n",
    "        text = tokenizer.decode(tokens, skip_special_tokens=True)\n",
    "        text = text.replace(\"n't\", \" not\")\n",
    "        # new_ids = prefix + t_tokenizer.encode(text)[1:] + [eos_token_id\n",
    "        new_ids = t_tokenizer.encode(text) + [eos_token_id]\n",
    "        \n",
    "        n_spans = []\n",
    "        n_tokenized = t_tokenizer.tokenize(text)\n",
    "        for i, span in enumerate(spans):\n",
    "            trigger, order = get_trigger(tokens, span)\n",
    "            trigger = format_trigger(trigger)\n",
    "            n_span = get_span(n_tokenized, trigger, order)\n",
    "            if not n_span:\n",
    "                # del labels[i]\n",
    "                print(trigger, span, order, text, tokens)\n",
    "                continue\n",
    "            \n",
    "            # count += 1\n",
    "            # if count > 10: break\n",
    "            # print(trigger, span, n_span, order, text, tokens)\n",
    "            # print(trigger, t_tokenizer.decode(new_ids[n_span[0]: n_span[1] + 1]))\n",
    "            n_spans.append(n_span)\n",
    "            # print(trigger, t_tokenizer.decode(new_ids[n_span[0]: n_span[1] + 1]), span, n_span, order, text, new_ids)\n",
    "        \n",
    "        # if count > 10: break\n",
    "        ignores = get_ignore(n_spans)\n",
    "        for i in range(len(prefix)+1, len(new_ids)-1):\n",
    "            if i not in ignores:\n",
    "                n_spans.append([i,i])\n",
    "                labels.append(0)\n",
    "\n",
    "        \n",
    "        row[\"piece_ids\"] = new_ids\n",
    "        row[\"span\"] = n_spans\n",
    "        row[\"label\"] = labels\n",
    "\n",
    "        json_record = json.dumps(row)\n",
    "        file.write(json_record + '\\n')\n",
    "\n",
    "# for row in data:\n",
    "#     tokens = row['piece_ids']\n",
    "#     spans = row['span']\n",
    "#     labels = row['label']\n",
    "\n",
    "#     labels, spans = get_valid(labels, spans)\n",
    "\n",
    "#     text = tokenizer.decode(tokens, skip_special_tokens=True)\n",
    "#     text = text.replace(\"n't\", \" not\")\n",
    "#     # new_ids = prefix + t_tokenizer.encode(text)[1:] + [eos_token_id\n",
    "#     new_ids = t_tokenizer.encode(text) + [eos_token_id]\n",
    "    \n",
    "#     n_spans = []\n",
    "#     n_tokenized = t_tokenizer.tokenize(text)\n",
    "#     for i, span in enumerate(spans):\n",
    "#         trigger, order = get_trigger(tokens, span)\n",
    "#         trigger = format_trigger(trigger)\n",
    "#         n_span = get_span(n_tokenized, trigger, order, len(prefix))\n",
    "#         if not n_span:\n",
    "#             # del labels[i]\n",
    "#             print(trigger, span, order, text, tokens)\n",
    "#             continue\n",
    "        \n",
    "#         # count += 1\n",
    "#         # if count > 10: break\n",
    "#         # print(trigger, span, n_span, order, text, tokens)\n",
    "#         # print(trigger, t_tokenizer.decode(new_ids[n_span[0]: n_span[1] + 1]))\n",
    "#         n_spans.append(n_span)\n",
    "    \n",
    "#     # if count > 10: break\n",
    "#     ignores = get_ignore(n_spans)\n",
    "#     for i in range(len(prefix), len(new_ids)):\n",
    "#         if i not in ignores:\n",
    "#             n_spans.append([i,i])\n",
    "#             labels.append(0)\n",
    "\n",
    "    \n",
    "#     row[\"piece_ids\"] = new_ids\n",
    "#     row[\"span\"] = n_spans\n",
    "#     row[\"label\"] = labels\n",
    "#     if len(new_ids) > 120:\n",
    "#         print(len(new_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' named'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_tokens = [128000, 32414, 819, 77275, 315, 279, 506, 1557, 16339, 342, 22231, 14198, 389, 259, 7192, 7086, 279, 1510, 2010, 315, 279, 3224, 596, 4907, 40704, 439, 279, 502, 21892, 315, 17452, 48011, 279, 6020, 3600, 11447, 482, 326, 10910, 482, 8789, 64, 482, 436, 10910, 43845, 128001]\n",
    "t_tokenizer.decode(n_tokens[15:16])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "eos_token_id = t_tokenizer.eos_token_id\n",
    "\n",
    "for perm_id in [0, 1, 2, 3, 4]:\n",
    "    for dir in os.listdir(f\"./data_incremental_2/ACE/perm{perm_id}\"):\n",
    "        data = load(f\"./data_incremental_2/ACE/perm{perm_id}/{dir}\")\n",
    "\n",
    "        prefix = t_tokenizer.encode(\"Classify the type of event corresponding to each token in the following sentence:\")\n",
    "        prefix = []\n",
    "\n",
    "        with open(f\"./data_incremental/ACE/perm{perm_id}/{dir}\", 'w') as file:\n",
    "            for task in data:\n",
    "                for key in task.keys():\n",
    "                    for row in task[key]:\n",
    "                        tokens = row['piece_ids']\n",
    "                        spans = row['span']\n",
    "                        labels = row['label']\n",
    "\n",
    "                        labels, spans = get_valid(labels, spans)\n",
    "\n",
    "                        text = tokenizer.decode(tokens, skip_special_tokens=True)\n",
    "                        text = text.replace(\"n't\", \" not\")\n",
    "                        # new_ids = prefix + t_tokenizer.encode(text)[1:] + [eos_token_id]\n",
    "                        new_ids = t_tokenizer.encode(text) + [eos_token_id]\n",
    "                        \n",
    "                        n_spans = []\n",
    "                        n_tokenized = t_tokenizer.tokenize(text)\n",
    "                        for i, span in enumerate(spans):\n",
    "                            trigger, order = get_trigger(tokens, span)\n",
    "                            trigger = format_trigger(trigger)\n",
    "                            n_span = get_span(n_tokenized, trigger, order)\n",
    "                            if not n_span:\n",
    "                                del labels[i]\n",
    "                                print(trigger, span, order, text, tokens)\n",
    "                                continue\n",
    "                            \n",
    "                            n_spans.append(n_span)\n",
    "                            # print(trigger, t_tokenizer.decode(new_ids[n_span[0]: n_span[1] + 1]))\n",
    "                        \n",
    "                        # if count > 10: break\n",
    "                        ignores = get_ignore(n_spans)\n",
    "                        for i in range(len(prefix)+1, len(new_ids)-1):\n",
    "                            if i not in ignores:\n",
    "                                n_spans.append([i,i])\n",
    "                                labels.append(0)\n",
    "\n",
    "                        \n",
    "                        row[\"piece_ids\"] = new_ids\n",
    "                        row[\"span\"] = n_spans\n",
    "                        row[\"label\"] = labels\n",
    "\n",
    "                json_record = json.dumps(task)\n",
    "                file.write(json_record + '\\n')\n",
    "\n",
    "# eos_token_id = t_tokenizer.eos_token_id\n",
    "\n",
    "# for perm_id in [0, 1, 2, 3, 4]:\n",
    "#     for dir in os.listdir(f\"./data_incremental_2/ACE/perm{perm_id}\"):\n",
    "#         data = load(f\"./data_incremental_2/ACE/perm{perm_id}/{dir}\")\n",
    "\n",
    "#         prefix = t_tokenizer.encode(\"Classify the type of event corresponding to each token in the following sentence:\")\n",
    "#         prefix = []\n",
    "\n",
    "#         for task in data:\n",
    "#             for key in task.keys():\n",
    "#                 for row in task[key]:\n",
    "#                     tokens = row['piece_ids']\n",
    "#                     spans = row['span']\n",
    "#                     labels = row['label']\n",
    "\n",
    "#                     labels, spans = get_valid(labels, spans)\n",
    "\n",
    "#                     text = tokenizer.decode(tokens, skip_special_tokens=True)\n",
    "#                     text = text.replace(\"n't\", \" not\")\n",
    "#                     # new_ids = prefix + t_tokenizer.encode(text)[1:] + [eos_token_id]\n",
    "#                     new_ids = t_tokenizer.encode(text) + [eos_token_id]\n",
    "                    \n",
    "#                     n_spans = []\n",
    "#                     n_tokenized = t_tokenizer.tokenize(text)\n",
    "#                     for i, span in enumerate(spans):\n",
    "#                         trigger, order = get_trigger(tokens, span)\n",
    "#                         trigger = format_trigger(trigger)\n",
    "#                         n_span = get_span(n_tokenized, trigger, order, len(prefix))\n",
    "#                         if not n_span:\n",
    "#                             del labels[i]\n",
    "#                             print(trigger, span, order, text, tokens)\n",
    "#                             continue\n",
    "                        \n",
    "#                         n_spans.append(n_span)\n",
    "                    \n",
    "#                     # if count > 10: break\n",
    "#                     ignores = get_ignore(n_spans)\n",
    "#                     for i in range(len(prefix), len(new_ids)):\n",
    "#                         if i not in ignores:\n",
    "#                             n_spans.append([i,i])\n",
    "#                             labels.append(0)\n",
    "\n",
    "                    \n",
    "#                     row[\"piece_ids\"] = new_ids\n",
    "#                     row[\"span\"] = n_spans\n",
    "#                     row[\"label\"] = labels\n",
    "\n",
    "#                     if len(new_ids) > 120:\n",
    "#                         print(len(new_ids))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int(\"23\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/anhnh/miniconda3/envs/commonenv/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from llm2vec import LLM2Vec\n",
    "\n",
    "l2v = LLM2Vec.from_pretrained(\n",
    "    \"McGill-NLP/LLM2Vec-Meta-Llama-3-8B-Instruct-mntp\",\n",
    "    peft_model_name_or_path=\"McGill-NLP/LLM2Vec-Meta-Llama-3-8B-Instruct-mntp-supervised\",\n",
    "    device_map=\"cuda:0\" if torch.cuda.is_available() else \"cpu\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    token=\"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\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = l2v.tokenize([\"i love you i love you<|end_of_text|>\", \"i love you i love you i love you\", \"i love you\"])\n",
    "del x['embed_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4096"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l2v.model.config.hidden_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|end_of_text|>'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[128001, 128001, 128000,     72,   3021,    499,    602,   3021,    499,\n",
       "         128001],\n",
       "        [128000,     72,   3021,    499,    602,   3021,    499,    602,   3021,\n",
       "            499],\n",
       "        [128001, 128001, 128001, 128001, 128001, 128001, 128000,     72,   3021,\n",
       "            499]]), 'attention_mask': tensor([[0, 0, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [0, 0, 0, 0, 0, 0, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedd = l2v.model(**(x.to(\"cuda:0\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedd.last_hidden_state.to(torch.bfloat16).dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "ln = torch.nn.Linear(2,3, dtype=torch.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[6, 6], [7, 8], [12, 12]]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "xx = [[1,1], [2,3], [7,7]]\n",
    "z = np.array(xx) + 5\n",
    "z.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/anhnh/miniconda3/envs/commonenv/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7e160022b6144ddb569ed2ec006d088",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a56437b6a6fe41d3a299c54a13cf3fd3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model's Lora trainable parameters:\n",
      "trainable params: 20,971,520 || all params: 7,525,896,192 || trainable%: 0.278658108814903\n"
     ]
    }
   ],
   "source": [
    "from model import LLM2VecED\n",
    "import torch\n",
    "\n",
    "model = LLM2VecED(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "prev_model = deepcopy(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'prev_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m prev_model\n\u001b[1;32m      2\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'prev_model' is not defined"
     ]
    }
   ],
   "source": [
    "del prev_model\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict = {k: v for k, v in model.state_dict().items() if 'backbone' not in k}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_IncompatibleKeys(missing_keys=['backbone.model.base_model.model.embed_tokens.weight', 'backbone.model.base_model.model.layers.0.self_attn.q_proj.base_layer.weight', 'backbone.model.base_model.model.layers.0.self_attn.q_proj.lora_A.default.weight', 'backbone.model.base_model.model.layers.0.self_attn.q_proj.lora_A.test.weight', 'backbone.model.base_model.model.layers.0.self_attn.q_proj.lora_B.default.weight', 'backbone.model.base_model.model.layers.0.self_attn.q_proj.lora_B.test.weight', 'backbone.model.base_model.model.layers.0.self_attn.k_proj.base_layer.weight', 'backbone.model.base_model.model.layers.0.self_attn.k_proj.lora_A.default.weight', 'backbone.model.base_model.model.layers.0.self_attn.k_proj.lora_A.test.weight', 'backbone.model.base_model.model.layers.0.self_attn.k_proj.lora_B.default.weight', 'backbone.model.base_model.model.layers.0.self_attn.k_proj.lora_B.test.weight', 'backbone.model.base_model.model.layers.0.self_attn.v_proj.base_layer.weight', 'backbone.model.base_model.model.layers.0.self_attn.v_proj.lora_A.default.weight', 'backbone.model.base_model.model.layers.0.self_attn.v_proj.lora_A.test.weight', 'backbone.model.base_model.model.layers.0.self_attn.v_proj.lora_B.default.weight', 'backbone.model.base_model.model.layers.0.self_attn.v_proj.lora_B.test.weight', 'backbone.model.base_model.model.layers.0.self_attn.o_proj.base_layer.weight', 'backbone.model.base_model.model.layers.0.self_attn.o_proj.lora_A.default.weight', 'backbone.model.base_model.model.layers.0.self_attn.o_proj.lora_A.test.weight', 'backbone.model.base_model.model.layers.0.self_attn.o_proj.lora_B.default.weight', 'backbone.model.base_model.model.layers.0.self_attn.o_proj.lora_B.test.weight', 'backbone.model.base_model.model.layers.0.mlp.gate_proj.base_layer.weight', 'backbone.model.base_model.model.layers.0.mlp.gate_proj.lora_A.default.weight', 'backbone.model.base_model.model.layers.0.mlp.gate_proj.lora_A.test.weight', 'backbone.model.base_model.model.layers.0.mlp.gate_proj.lora_B.default.weight', 'backbone.model.base_model.model.layers.0.mlp.gate_proj.lora_B.test.weight', 'backbone.model.base_model.model.layers.0.mlp.up_proj.base_layer.weight', 'backbone.model.base_model.model.layers.0.mlp.up_proj.lora_A.default.weight', 'backbone.model.base_model.model.layers.0.mlp.up_proj.lora_A.test.weight', 'backbone.model.base_model.model.layers.0.mlp.up_proj.lora_B.default.weight', 'backbone.model.base_model.model.layers.0.mlp.up_proj.lora_B.test.weight', 'backbone.model.base_model.model.layers.0.mlp.down_proj.base_layer.weight', 'backbone.model.base_model.model.layers.0.mlp.down_proj.lora_A.default.weight', 'backbone.model.base_model.model.layers.0.mlp.down_proj.lora_A.test.weight', 'backbone.model.base_model.model.layers.0.mlp.down_proj.lora_B.default.weight', 'backbone.model.base_model.model.layers.0.mlp.down_proj.lora_B.test.weight', 'backbone.model.base_model.model.layers.0.input_layernorm.weight', 'backbone.model.base_model.model.layers.0.post_attention_layernorm.weight', 'backbone.model.base_model.model.layers.1.self_attn.q_proj.base_layer.weight', 'backbone.model.base_model.model.layers.1.self_attn.q_proj.lora_A.default.weight', 'backbone.model.base_model.model.layers.1.self_attn.q_proj.lora_A.test.weight', 'backbone.model.base_model.model.layers.1.self_attn.q_proj.lora_B.default.weight', 'backbone.model.base_model.model.layers.1.self_attn.q_proj.lora_B.test.weight', 'backbone.model.base_model.model.layers.1.self_attn.k_proj.base_layer.weight', 'backbone.model.base_model.model.layers.1.self_attn.k_proj.lora_A.default.weight', 'backbone.model.base_model.model.layers.1.self_attn.k_proj.lora_A.test.weight', 'backbone.model.base_model.model.layers.1.self_attn.k_proj.lora_B.default.weight', 'backbone.model.base_model.model.layers.1.self_attn.k_proj.lora_B.test.weight', 'backbone.model.base_model.model.layers.1.self_attn.v_proj.base_layer.weight', 'backbone.model.base_model.model.layers.1.self_attn.v_proj.lora_A.default.weight', 'backbone.model.base_model.model.layers.1.self_attn.v_proj.lora_A.test.weight', 'backbone.model.base_model.model.layers.1.self_attn.v_proj.lora_B.default.weight', 'backbone.model.base_model.model.layers.1.self_attn.v_proj.lora_B.test.weight', 'backbone.model.base_model.model.layers.1.self_attn.o_proj.base_layer.weight', 'backbone.model.base_model.model.layers.1.self_attn.o_proj.lora_A.default.weight', 'backbone.model.base_model.model.layers.1.self_attn.o_proj.lora_A.test.weight', 'backbone.model.base_model.model.layers.1.self_attn.o_proj.lora_B.default.weight', 'backbone.model.base_model.model.layers.1.self_attn.o_proj.lora_B.test.weight', 'backbone.model.base_model.model.layers.1.mlp.gate_proj.base_layer.weight', 'backbone.model.base_model.model.layers.1.mlp.gate_proj.lora_A.default.weight', 'backbone.model.base_model.model.layers.1.mlp.gate_proj.lora_A.test.weight', 'backbone.model.base_model.model.layers.1.mlp.gate_proj.lora_B.default.weight', 'backbone.model.base_model.model.layers.1.mlp.gate_proj.lora_B.test.weight', 'backbone.model.base_model.model.layers.1.mlp.up_proj.base_layer.weight', 'backbone.model.base_model.model.layers.1.mlp.up_proj.lora_A.default.weight', 'backbone.model.base_model.model.layers.1.mlp.up_proj.lora_A.test.weight', 'backbone.model.base_model.model.layers.1.mlp.up_proj.lora_B.default.weight', 'backbone.model.base_model.model.layers.1.mlp.up_proj.lora_B.test.weight', 'backbone.model.base_model.model.layers.1.mlp.down_proj.base_layer.weight', 'backbone.model.base_model.model.layers.1.mlp.down_proj.lora_A.default.weight', 'backbone.model.base_model.model.layers.1.mlp.down_proj.lora_A.test.weight', 'backbone.model.base_model.model.layers.1.mlp.down_proj.lora_B.default.weight', 'backbone.model.base_model.model.layers.1.mlp.down_proj.lora_B.test.weight', 'backbone.model.base_model.model.layers.1.input_layernorm.weight', 'backbone.model.base_model.model.layers.1.post_attention_layernorm.weight', 'backbone.model.base_model.model.layers.2.self_attn.q_proj.base_layer.weight', 'backbone.model.base_model.model.layers.2.self_attn.q_proj.lora_A.default.weight', 'backbone.model.base_model.model.layers.2.self_attn.q_proj.lora_A.test.weight', 'backbone.model.base_model.model.layers.2.self_attn.q_proj.lora_B.default.weight', 'backbone.model.base_model.model.layers.2.self_attn.q_proj.lora_B.test.weight', 'backbone.model.base_model.model.layers.2.self_attn.k_proj.base_layer.weight', 'backbone.model.base_model.model.layers.2.self_attn.k_proj.lora_A.default.weight', 'backbone.model.base_model.model.layers.2.self_attn.k_proj.lora_A.test.weight', 'backbone.model.base_model.model.layers.2.self_attn.k_proj.lora_B.default.weight', 'backbone.model.base_model.model.layers.2.self_attn.k_proj.lora_B.test.weight', 'backbone.model.base_model.model.layers.2.self_attn.v_proj.base_layer.weight', 'backbone.model.base_model.model.layers.2.self_attn.v_proj.lora_A.default.weight', 'backbone.model.base_model.model.layers.2.self_attn.v_proj.lora_A.test.weight', 'backbone.model.base_model.model.layers.2.self_attn.v_proj.lora_B.default.weight', 'backbone.model.base_model.model.layers.2.self_attn.v_proj.lora_B.test.weight', 'backbone.model.base_model.model.layers.2.self_attn.o_proj.base_layer.weight', 'backbone.model.base_model.model.layers.2.self_attn.o_proj.lora_A.default.weight', 'backbone.model.base_model.model.layers.2.self_attn.o_proj.lora_A.test.weight', 'backbone.model.base_model.model.layers.2.self_attn.o_proj.lora_B.default.weight', 'backbone.model.base_model.model.layers.2.self_attn.o_proj.lora_B.test.weight', 'backbone.model.base_model.model.layers.2.mlp.gate_proj.base_layer.weight', 'backbone.model.base_model.model.layers.2.mlp.gate_proj.lora_A.default.weight', 'backbone.model.base_model.model.layers.2.mlp.gate_proj.lora_A.test.weight', 'backbone.model.base_model.model.layers.2.mlp.gate_proj.lora_B.default.weight', 'backbone.model.base_model.model.layers.2.mlp.gate_proj.lora_B.test.weight', 'backbone.model.base_model.model.layers.2.mlp.up_proj.base_layer.weight', 'backbone.model.base_model.model.layers.2.mlp.up_proj.lora_A.default.weight', 'backbone.model.base_model.model.layers.2.mlp.up_proj.lora_A.test.weight', 'backbone.model.base_model.model.layers.2.mlp.up_proj.lora_B.default.weight', 'backbone.model.base_model.model.layers.2.mlp.up_proj.lora_B.test.weight', 'backbone.model.base_model.model.layers.2.mlp.down_proj.base_layer.weight', 'backbone.model.base_model.model.layers.2.mlp.down_proj.lora_A.default.weight', 'backbone.model.base_model.model.layers.2.mlp.down_proj.lora_A.test.weight', 'backbone.model.base_model.model.layers.2.mlp.down_proj.lora_B.default.weight', 'backbone.model.base_model.model.layers.2.mlp.down_proj.lora_B.test.weight', 'backbone.model.base_model.model.layers.2.input_layernorm.weight', 'backbone.model.base_model.model.layers.2.post_attention_layernorm.weight', 'backbone.model.base_model.model.layers.3.self_attn.q_proj.base_layer.weight', 'backbone.model.base_model.model.layers.3.self_attn.q_proj.lora_A.default.weight', 'backbone.model.base_model.model.layers.3.self_attn.q_proj.lora_A.test.weight', 'backbone.model.base_model.model.layers.3.self_attn.q_proj.lora_B.default.weight', 'backbone.model.base_model.model.layers.3.self_attn.q_proj.lora_B.test.weight', 'backbone.model.base_model.model.layers.3.self_attn.k_proj.base_layer.weight', 'backbone.model.base_model.model.layers.3.self_attn.k_proj.lora_A.default.weight', 'backbone.model.base_model.model.layers.3.self_attn.k_proj.lora_A.test.weight', 'backbone.model.base_model.model.layers.3.self_attn.k_proj.lora_B.default.weight', 'backbone.model.base_model.model.layers.3.self_attn.k_proj.lora_B.test.weight', 'backbone.model.base_model.model.layers.3.self_attn.v_proj.base_layer.weight', 'backbone.model.base_model.model.layers.3.self_attn.v_proj.lora_A.default.weight', 'backbone.model.base_model.model.layers.3.self_attn.v_proj.lora_A.test.weight', 'backbone.model.base_model.model.layers.3.self_attn.v_proj.lora_B.default.weight', 'backbone.model.base_model.model.layers.3.self_attn.v_proj.lora_B.test.weight', 'backbone.model.base_model.model.layers.3.self_attn.o_proj.base_layer.weight', 'backbone.model.base_model.model.layers.3.self_attn.o_proj.lora_A.default.weight', 'backbone.model.base_model.model.layers.3.self_attn.o_proj.lora_A.test.weight', 'backbone.model.base_model.model.layers.3.self_attn.o_proj.lora_B.default.weight', 'backbone.model.base_model.model.layers.3.self_attn.o_proj.lora_B.test.weight', 'backbone.model.base_model.model.layers.3.mlp.gate_proj.base_layer.weight', 'backbone.model.base_model.model.layers.3.mlp.gate_proj.lora_A.default.weight', 'backbone.model.base_model.model.layers.3.mlp.gate_proj.lora_A.test.weight', 'backbone.model.base_model.model.layers.3.mlp.gate_proj.lora_B.default.weight', 'backbone.model.base_model.model.layers.3.mlp.gate_proj.lora_B.test.weight', 'backbone.model.base_model.model.layers.3.mlp.up_proj.base_layer.weight', 'backbone.model.base_model.model.layers.3.mlp.up_proj.lora_A.default.weight', 'backbone.model.base_model.model.layers.3.mlp.up_proj.lora_A.test.weight', 'backbone.model.base_model.model.layers.3.mlp.up_proj.lora_B.default.weight', 'backbone.model.base_model.model.layers.3.mlp.up_proj.lora_B.test.weight', 'backbone.model.base_model.model.layers.3.mlp.down_proj.base_layer.weight', 'backbone.model.base_model.model.layers.3.mlp.down_proj.lora_A.default.weight', 'backbone.model.base_model.model.layers.3.mlp.down_proj.lora_A.test.weight', 'backbone.model.base_model.model.layers.3.mlp.down_proj.lora_B.default.weight', 'backbone.model.base_model.model.layers.3.mlp.down_proj.lora_B.test.weight', 'backbone.model.base_model.model.layers.3.input_layernorm.weight', 'backbone.model.base_model.model.layers.3.post_attention_layernorm.weight', 'backbone.model.base_model.model.layers.4.self_attn.q_proj.base_layer.weight', 'backbone.model.base_model.model.layers.4.self_attn.q_proj.lora_A.default.weight', 'backbone.model.base_model.model.layers.4.self_attn.q_proj.lora_A.test.weight', 'backbone.model.base_model.model.layers.4.self_attn.q_proj.lora_B.default.weight', 'backbone.model.base_model.model.layers.4.self_attn.q_proj.lora_B.test.weight', 'backbone.model.base_model.model.layers.4.self_attn.k_proj.base_layer.weight', 'backbone.model.base_model.model.layers.4.self_attn.k_proj.lora_A.default.weight', 'backbone.model.base_model.model.layers.4.self_attn.k_proj.lora_A.test.weight', 'backbone.model.base_model.model.layers.4.self_attn.k_proj.lora_B.default.weight', 'backbone.model.base_model.model.layers.4.self_attn.k_proj.lora_B.test.weight', 'backbone.model.base_model.model.layers.4.self_attn.v_proj.base_layer.weight', 'backbone.model.base_model.model.layers.4.self_attn.v_proj.lora_A.default.weight', 'backbone.model.base_model.model.layers.4.self_attn.v_proj.lora_A.test.weight', 'backbone.model.base_model.model.layers.4.self_attn.v_proj.lora_B.default.weight', 'backbone.model.base_model.model.layers.4.self_attn.v_proj.lora_B.test.weight', 'backbone.model.base_model.model.layers.4.self_attn.o_proj.base_layer.weight', 'backbone.model.base_model.model.layers.4.self_attn.o_proj.lora_A.default.weight', 'backbone.model.base_model.model.layers.4.self_attn.o_proj.lora_A.test.weight', 'backbone.model.base_model.model.layers.4.self_attn.o_proj.lora_B.default.weight', 'backbone.model.base_model.model.layers.4.self_attn.o_proj.lora_B.test.weight', 'backbone.model.base_model.model.layers.4.mlp.gate_proj.base_layer.weight', 'backbone.model.base_model.model.layers.4.mlp.gate_proj.lora_A.default.weight', 'backbone.model.base_model.model.layers.4.mlp.gate_proj.lora_A.test.weight', 'backbone.model.base_model.model.layers.4.mlp.gate_proj.lora_B.default.weight', 'backbone.model.base_model.model.layers.4.mlp.gate_proj.lora_B.test.weight', 'backbone.model.base_model.model.layers.4.mlp.up_proj.base_layer.weight', 'backbone.model.base_model.model.layers.4.mlp.up_proj.lora_A.default.weight', 'backbone.model.base_model.model.layers.4.mlp.up_proj.lora_A.test.weight', 'backbone.model.base_model.model.layers.4.mlp.up_proj.lora_B.default.weight', 'backbone.model.base_model.model.layers.4.mlp.up_proj.lora_B.test.weight', 'backbone.model.base_model.model.layers.4.mlp.down_proj.base_layer.weight', 'backbone.model.base_model.model.layers.4.mlp.down_proj.lora_A.default.weight', 'backbone.model.base_model.model.layers.4.mlp.down_proj.lora_A.test.weight', 'backbone.model.base_model.model.layers.4.mlp.down_proj.lora_B.default.weight', 'backbone.model.base_model.model.layers.4.mlp.down_proj.lora_B.test.weight', 'backbone.model.base_model.model.layers.4.input_layernorm.weight', 'backbone.model.base_model.model.layers.4.post_attention_layernorm.weight', 'backbone.model.base_model.model.layers.5.self_attn.q_proj.base_layer.weight', 'backbone.model.base_model.model.layers.5.self_attn.q_proj.lora_A.default.weight', 'backbone.model.base_model.model.layers.5.self_attn.q_proj.lora_A.test.weight', 'backbone.model.base_model.model.layers.5.self_attn.q_proj.lora_B.default.weight', 'backbone.model.base_model.model.layers.5.self_attn.q_proj.lora_B.test.weight', 'backbone.model.base_model.model.layers.5.self_attn.k_proj.base_layer.weight', 'backbone.model.base_model.model.layers.5.self_attn.k_proj.lora_A.default.weight', 'backbone.model.base_model.model.layers.5.self_attn.k_proj.lora_A.test.weight', 'backbone.model.base_model.model.layers.5.self_attn.k_proj.lora_B.default.weight', 'backbone.model.base_model.model.layers.5.self_attn.k_proj.lora_B.test.weight', 'backbone.model.base_model.model.layers.5.self_attn.v_proj.base_layer.weight', 'backbone.model.base_model.model.layers.5.self_attn.v_proj.lora_A.default.weight', 'backbone.model.base_model.model.layers.5.self_attn.v_proj.lora_A.test.weight', 'backbone.model.base_model.model.layers.5.self_attn.v_proj.lora_B.default.weight', 'backbone.model.base_model.model.layers.5.self_attn.v_proj.lora_B.test.weight', 'backbone.model.base_model.model.layers.5.self_attn.o_proj.base_layer.weight', 'backbone.model.base_model.model.layers.5.self_attn.o_proj.lora_A.default.weight', 'backbone.model.base_model.model.layers.5.self_attn.o_proj.lora_A.test.weight', 'backbone.model.base_model.model.layers.5.self_attn.o_proj.lora_B.default.weight', 'backbone.model.base_model.model.layers.5.self_attn.o_proj.lora_B.test.weight', 'backbone.model.base_model.model.layers.5.mlp.gate_proj.base_layer.weight', 'backbone.model.base_model.model.layers.5.mlp.gate_proj.lora_A.default.weight', 'backbone.model.base_model.model.layers.5.mlp.gate_proj.lora_A.test.weight', 'backbone.model.base_model.model.layers.5.mlp.gate_proj.lora_B.default.weight', 'backbone.model.base_model.model.layers.5.mlp.gate_proj.lora_B.test.weight', 'backbone.model.base_model.model.layers.5.mlp.up_proj.base_layer.weight', 'backbone.model.base_model.model.layers.5.mlp.up_proj.lora_A.default.weight', 'backbone.model.base_model.model.layers.5.mlp.up_proj.lora_A.test.weight', 'backbone.model.base_model.model.layers.5.mlp.up_proj.lora_B.default.weight', 'backbone.model.base_model.model.layers.5.mlp.up_proj.lora_B.test.weight', 'backbone.model.base_model.model.layers.5.mlp.down_proj.base_layer.weight', 'backbone.model.base_model.model.layers.5.mlp.down_proj.lora_A.default.weight', 'backbone.model.base_model.model.layers.5.mlp.down_proj.lora_A.test.weight', 'backbone.model.base_model.model.layers.5.mlp.down_proj.lora_B.default.weight', 'backbone.model.base_model.model.layers.5.mlp.down_proj.lora_B.test.weight', 'backbone.model.base_model.model.layers.5.input_layernorm.weight', 'backbone.model.base_model.model.layers.5.post_attention_layernorm.weight', 'backbone.model.base_model.model.layers.6.self_attn.q_proj.base_layer.weight', 'backbone.model.base_model.model.layers.6.self_attn.q_proj.lora_A.default.weight', 'backbone.model.base_model.model.layers.6.self_attn.q_proj.lora_A.test.weight', 'backbone.model.base_model.model.layers.6.self_attn.q_proj.lora_B.default.weight', 'backbone.model.base_model.model.layers.6.self_attn.q_proj.lora_B.test.weight', 'backbone.model.base_model.model.layers.6.self_attn.k_proj.base_layer.weight', 'backbone.model.base_model.model.layers.6.self_attn.k_proj.lora_A.default.weight', 'backbone.model.base_model.model.layers.6.self_attn.k_proj.lora_A.test.weight', 'backbone.model.base_model.model.layers.6.self_attn.k_proj.lora_B.default.weight', 'backbone.model.base_model.model.layers.6.self_attn.k_proj.lora_B.test.weight', 'backbone.model.base_model.model.layers.6.self_attn.v_proj.base_layer.weight', 'backbone.model.base_model.model.layers.6.self_attn.v_proj.lora_A.default.weight', 'backbone.model.base_model.model.layers.6.self_attn.v_proj.lora_A.test.weight', 'backbone.model.base_model.model.layers.6.self_attn.v_proj.lora_B.default.weight', 'backbone.model.base_model.model.layers.6.self_attn.v_proj.lora_B.test.weight', 'backbone.model.base_model.model.layers.6.self_attn.o_proj.base_layer.weight', 'backbone.model.base_model.model.layers.6.self_attn.o_proj.lora_A.default.weight', 'backbone.model.base_model.model.layers.6.self_attn.o_proj.lora_A.test.weight', 'backbone.model.base_model.model.layers.6.self_attn.o_proj.lora_B.default.weight', 'backbone.model.base_model.model.layers.6.self_attn.o_proj.lora_B.test.weight', 'backbone.model.base_model.model.layers.6.mlp.gate_proj.base_layer.weight', 'backbone.model.base_model.model.layers.6.mlp.gate_proj.lora_A.default.weight', 'backbone.model.base_model.model.layers.6.mlp.gate_proj.lora_A.test.weight', 'backbone.model.base_model.model.layers.6.mlp.gate_proj.lora_B.default.weight', 'backbone.model.base_model.model.layers.6.mlp.gate_proj.lora_B.test.weight', 'backbone.model.base_model.model.layers.6.mlp.up_proj.base_layer.weight', 'backbone.model.base_model.model.layers.6.mlp.up_proj.lora_A.default.weight', 'backbone.model.base_model.model.layers.6.mlp.up_proj.lora_A.test.weight', 'backbone.model.base_model.model.layers.6.mlp.up_proj.lora_B.default.weight', 'backbone.model.base_model.model.layers.6.mlp.up_proj.lora_B.test.weight', 'backbone.model.base_model.model.layers.6.mlp.down_proj.base_layer.weight', 'backbone.model.base_model.model.layers.6.mlp.down_proj.lora_A.default.weight', 'backbone.model.base_model.model.layers.6.mlp.down_proj.lora_A.test.weight', 'backbone.model.base_model.model.layers.6.mlp.down_proj.lora_B.default.weight', 'backbone.model.base_model.model.layers.6.mlp.down_proj.lora_B.test.weight', 'backbone.model.base_model.model.layers.6.input_layernorm.weight', 'backbone.model.base_model.model.layers.6.post_attention_layernorm.weight', 'backbone.model.base_model.model.layers.7.self_attn.q_proj.base_layer.weight', 'backbone.model.base_model.model.layers.7.self_attn.q_proj.lora_A.default.weight', 'backbone.model.base_model.model.layers.7.self_attn.q_proj.lora_A.test.weight', 'backbone.model.base_model.model.layers.7.self_attn.q_proj.lora_B.default.weight', 'backbone.model.base_model.model.layers.7.self_attn.q_proj.lora_B.test.weight', 'backbone.model.base_model.model.layers.7.self_attn.k_proj.base_layer.weight', 'backbone.model.base_model.model.layers.7.self_attn.k_proj.lora_A.default.weight', 'backbone.model.base_model.model.layers.7.self_attn.k_proj.lora_A.test.weight', 'backbone.model.base_model.model.layers.7.self_attn.k_proj.lora_B.default.weight', 'backbone.model.base_model.model.layers.7.self_attn.k_proj.lora_B.test.weight', 'backbone.model.base_model.model.layers.7.self_attn.v_proj.base_layer.weight', 'backbone.model.base_model.model.layers.7.self_attn.v_proj.lora_A.default.weight', 'backbone.model.base_model.model.layers.7.self_attn.v_proj.lora_A.test.weight', 'backbone.model.base_model.model.layers.7.self_attn.v_proj.lora_B.default.weight', 'backbone.model.base_model.model.layers.7.self_attn.v_proj.lora_B.test.weight', 'backbone.model.base_model.model.layers.7.self_attn.o_proj.base_layer.weight', 'backbone.model.base_model.model.layers.7.self_attn.o_proj.lora_A.default.weight', 'backbone.model.base_model.model.layers.7.self_attn.o_proj.lora_A.test.weight', 'backbone.model.base_model.model.layers.7.self_attn.o_proj.lora_B.default.weight', 'backbone.model.base_model.model.layers.7.self_attn.o_proj.lora_B.test.weight', 'backbone.model.base_model.model.layers.7.mlp.gate_proj.base_layer.weight', 'backbone.model.base_model.model.layers.7.mlp.gate_proj.lora_A.default.weight', 'backbone.model.base_model.model.layers.7.mlp.gate_proj.lora_A.test.weight', 'backbone.model.base_model.model.layers.7.mlp.gate_proj.lora_B.default.weight', 'backbone.model.base_model.model.layers.7.mlp.gate_proj.lora_B.test.weight', 'backbone.model.base_model.model.layers.7.mlp.up_proj.base_layer.weight', 'backbone.model.base_model.model.layers.7.mlp.up_proj.lora_A.default.weight', 'backbone.model.base_model.model.layers.7.mlp.up_proj.lora_A.test.weight', 'backbone.model.base_model.model.layers.7.mlp.up_proj.lora_B.default.weight', 'backbone.model.base_model.model.layers.7.mlp.up_proj.lora_B.test.weight', 'backbone.model.base_model.model.layers.7.mlp.down_proj.base_layer.weight', 'backbone.model.base_model.model.layers.7.mlp.down_proj.lora_A.default.weight', 'backbone.model.base_model.model.layers.7.mlp.down_proj.lora_A.test.weight', 'backbone.model.base_model.model.layers.7.mlp.down_proj.lora_B.default.weight', 'backbone.model.base_model.model.layers.7.mlp.down_proj.lora_B.test.weight', 'backbone.model.base_model.model.layers.7.input_layernorm.weight', 'backbone.model.base_model.model.layers.7.post_attention_layernorm.weight', 'backbone.model.base_model.model.layers.8.self_attn.q_proj.base_layer.weight', 'backbone.model.base_model.model.layers.8.self_attn.q_proj.lora_A.default.weight', 'backbone.model.base_model.model.layers.8.self_attn.q_proj.lora_A.test.weight', 'backbone.model.base_model.model.layers.8.self_attn.q_proj.lora_B.default.weight', 'backbone.model.base_model.model.layers.8.self_attn.q_proj.lora_B.test.weight', 'backbone.model.base_model.model.layers.8.self_attn.k_proj.base_layer.weight', 'backbone.model.base_model.model.layers.8.self_attn.k_proj.lora_A.default.weight', 'backbone.model.base_model.model.layers.8.self_attn.k_proj.lora_A.test.weight', 'backbone.model.base_model.model.layers.8.self_attn.k_proj.lora_B.default.weight', 'backbone.model.base_model.model.layers.8.self_attn.k_proj.lora_B.test.weight', 'backbone.model.base_model.model.layers.8.self_attn.v_proj.base_layer.weight', 'backbone.model.base_model.model.layers.8.self_attn.v_proj.lora_A.default.weight', 'backbone.model.base_model.model.layers.8.self_attn.v_proj.lora_A.test.weight', 'backbone.model.base_model.model.layers.8.self_attn.v_proj.lora_B.default.weight', 'backbone.model.base_model.model.layers.8.self_attn.v_proj.lora_B.test.weight', 'backbone.model.base_model.model.layers.8.self_attn.o_proj.base_layer.weight', 'backbone.model.base_model.model.layers.8.self_attn.o_proj.lora_A.default.weight', 'backbone.model.base_model.model.layers.8.self_attn.o_proj.lora_A.test.weight', 'backbone.model.base_model.model.layers.8.self_attn.o_proj.lora_B.default.weight', 'backbone.model.base_model.model.layers.8.self_attn.o_proj.lora_B.test.weight', 'backbone.model.base_model.model.layers.8.mlp.gate_proj.base_layer.weight', 'backbone.model.base_model.model.layers.8.mlp.gate_proj.lora_A.default.weight', 'backbone.model.base_model.model.layers.8.mlp.gate_proj.lora_A.test.weight', 'backbone.model.base_model.model.layers.8.mlp.gate_proj.lora_B.default.weight', 'backbone.model.base_model.model.layers.8.mlp.gate_proj.lora_B.test.weight', 'backbone.model.base_model.model.layers.8.mlp.up_proj.base_layer.weight', 'backbone.model.base_model.model.layers.8.mlp.up_proj.lora_A.default.weight', 'backbone.model.base_model.model.layers.8.mlp.up_proj.lora_A.test.weight', 'backbone.model.base_model.model.layers.8.mlp.up_proj.lora_B.default.weight', 'backbone.model.base_model.model.layers.8.mlp.up_proj.lora_B.test.weight', 'backbone.model.base_model.model.layers.8.mlp.down_proj.base_layer.weight', 'backbone.model.base_model.model.layers.8.mlp.down_proj.lora_A.default.weight', 'backbone.model.base_model.model.layers.8.mlp.down_proj.lora_A.test.weight', 'backbone.model.base_model.model.layers.8.mlp.down_proj.lora_B.default.weight', 'backbone.model.base_model.model.layers.8.mlp.down_proj.lora_B.test.weight', 'backbone.model.base_model.model.layers.8.input_layernorm.weight', 'backbone.model.base_model.model.layers.8.post_attention_layernorm.weight', 'backbone.model.base_model.model.layers.9.self_attn.q_proj.base_layer.weight', 'backbone.model.base_model.model.layers.9.self_attn.q_proj.lora_A.default.weight', 'backbone.model.base_model.model.layers.9.self_attn.q_proj.lora_A.test.weight', 'backbone.model.base_model.model.layers.9.self_attn.q_proj.lora_B.default.weight', 'backbone.model.base_model.model.layers.9.self_attn.q_proj.lora_B.test.weight', 'backbone.model.base_model.model.layers.9.self_attn.k_proj.base_layer.weight', 'backbone.model.base_model.model.layers.9.self_attn.k_proj.lora_A.default.weight', 'backbone.model.base_model.model.layers.9.self_attn.k_proj.lora_A.test.weight', 'backbone.model.base_model.model.layers.9.self_attn.k_proj.lora_B.default.weight', 'backbone.model.base_model.model.layers.9.self_attn.k_proj.lora_B.test.weight', 'backbone.model.base_model.model.layers.9.self_attn.v_proj.base_layer.weight', 'backbone.model.base_model.model.layers.9.self_attn.v_proj.lora_A.default.weight', 'backbone.model.base_model.model.layers.9.self_attn.v_proj.lora_A.test.weight', 'backbone.model.base_model.model.layers.9.self_attn.v_proj.lora_B.default.weight', 'backbone.model.base_model.model.layers.9.self_attn.v_proj.lora_B.test.weight', 'backbone.model.base_model.model.layers.9.self_attn.o_proj.base_layer.weight', 'backbone.model.base_model.model.layers.9.self_attn.o_proj.lora_A.default.weight', 'backbone.model.base_model.model.layers.9.self_attn.o_proj.lora_A.test.weight', 'backbone.model.base_model.model.layers.9.self_attn.o_proj.lora_B.default.weight', 'backbone.model.base_model.model.layers.9.self_attn.o_proj.lora_B.test.weight', 'backbone.model.base_model.model.layers.9.mlp.gate_proj.base_layer.weight', 'backbone.model.base_model.model.layers.9.mlp.gate_proj.lora_A.default.weight', 'backbone.model.base_model.model.layers.9.mlp.gate_proj.lora_A.test.weight', 'backbone.model.base_model.model.layers.9.mlp.gate_proj.lora_B.default.weight', 'backbone.model.base_model.model.layers.9.mlp.gate_proj.lora_B.test.weight', 'backbone.model.base_model.model.layers.9.mlp.up_proj.base_layer.weight', 'backbone.model.base_model.model.layers.9.mlp.up_proj.lora_A.default.weight', 'backbone.model.base_model.model.layers.9.mlp.up_proj.lora_A.test.weight', 'backbone.model.base_model.model.layers.9.mlp.up_proj.lora_B.default.weight', 'backbone.model.base_model.model.layers.9.mlp.up_proj.lora_B.test.weight', 'backbone.model.base_model.model.layers.9.mlp.down_proj.base_layer.weight', 'backbone.model.base_model.model.layers.9.mlp.down_proj.lora_A.default.weight', 'backbone.model.base_model.model.layers.9.mlp.down_proj.lora_A.test.weight', 'backbone.model.base_model.model.layers.9.mlp.down_proj.lora_B.default.weight', 'backbone.model.base_model.model.layers.9.mlp.down_proj.lora_B.test.weight', 'backbone.model.base_model.model.layers.9.input_layernorm.weight', 'backbone.model.base_model.model.layers.9.post_attention_layernorm.weight', 'backbone.model.base_model.model.layers.10.self_attn.q_proj.base_layer.weight', 'backbone.model.base_model.model.layers.10.self_attn.q_proj.lora_A.default.weight', 'backbone.model.base_model.model.layers.10.self_attn.q_proj.lora_A.test.weight', 'backbone.model.base_model.model.layers.10.self_attn.q_proj.lora_B.default.weight', 'backbone.model.base_model.model.layers.10.self_attn.q_proj.lora_B.test.weight', 'backbone.model.base_model.model.layers.10.self_attn.k_proj.base_layer.weight', 'backbone.model.base_model.model.layers.10.self_attn.k_proj.lora_A.default.weight', 'backbone.model.base_model.model.layers.10.self_attn.k_proj.lora_A.test.weight', 'backbone.model.base_model.model.layers.10.self_attn.k_proj.lora_B.default.weight', 'backbone.model.base_model.model.layers.10.self_attn.k_proj.lora_B.test.weight', 'backbone.model.base_model.model.layers.10.self_attn.v_proj.base_layer.weight', 'backbone.model.base_model.model.layers.10.self_attn.v_proj.lora_A.default.weight', 'backbone.model.base_model.model.layers.10.self_attn.v_proj.lora_A.test.weight', 'backbone.model.base_model.model.layers.10.self_attn.v_proj.lora_B.default.weight', 'backbone.model.base_model.model.layers.10.self_attn.v_proj.lora_B.test.weight', 'backbone.model.base_model.model.layers.10.self_attn.o_proj.base_layer.weight', 'backbone.model.base_model.model.layers.10.self_attn.o_proj.lora_A.default.weight', 'backbone.model.base_model.model.layers.10.self_attn.o_proj.lora_A.test.weight', 'backbone.model.base_model.model.layers.10.self_attn.o_proj.lora_B.default.weight', 'backbone.model.base_model.model.layers.10.self_attn.o_proj.lora_B.test.weight', 'backbone.model.base_model.model.layers.10.mlp.gate_proj.base_layer.weight', 'backbone.model.base_model.model.layers.10.mlp.gate_proj.lora_A.default.weight', 'backbone.model.base_model.model.layers.10.mlp.gate_proj.lora_A.test.weight', 'backbone.model.base_model.model.layers.10.mlp.gate_proj.lora_B.default.weight', 'backbone.model.base_model.model.layers.10.mlp.gate_proj.lora_B.test.weight', 'backbone.model.base_model.model.layers.10.mlp.up_proj.base_layer.weight', 'backbone.model.base_model.model.layers.10.mlp.up_proj.lora_A.default.weight', 'backbone.model.base_model.model.layers.10.mlp.up_proj.lora_A.test.weight', 'backbone.model.base_model.model.layers.10.mlp.up_proj.lora_B.default.weight', 'backbone.model.base_model.model.layers.10.mlp.up_proj.lora_B.test.weight', 'backbone.model.base_model.model.layers.10.mlp.down_proj.base_layer.weight', 'backbone.model.base_model.model.layers.10.mlp.down_proj.lora_A.default.weight', 'backbone.model.base_model.model.layers.10.mlp.down_proj.lora_A.test.weight', 'backbone.model.base_model.model.layers.10.mlp.down_proj.lora_B.default.weight', 'backbone.model.base_model.model.layers.10.mlp.down_proj.lora_B.test.weight', 'backbone.model.base_model.model.layers.10.input_layernorm.weight', 'backbone.model.base_model.model.layers.10.post_attention_layernorm.weight', 'backbone.model.base_model.model.layers.11.self_attn.q_proj.base_layer.weight', 'backbone.model.base_model.model.layers.11.self_attn.q_proj.lora_A.default.weight', 'backbone.model.base_model.model.layers.11.self_attn.q_proj.lora_A.test.weight', 'backbone.model.base_model.model.layers.11.self_attn.q_proj.lora_B.default.weight', 'backbone.model.base_model.model.layers.11.self_attn.q_proj.lora_B.test.weight', 'backbone.model.base_model.model.layers.11.self_attn.k_proj.base_layer.weight', 'backbone.model.base_model.model.layers.11.self_attn.k_proj.lora_A.default.weight', 'backbone.model.base_model.model.layers.11.self_attn.k_proj.lora_A.test.weight', 'backbone.model.base_model.model.layers.11.self_attn.k_proj.lora_B.default.weight', 'backbone.model.base_model.model.layers.11.self_attn.k_proj.lora_B.test.weight', 'backbone.model.base_model.model.layers.11.self_attn.v_proj.base_layer.weight', 'backbone.model.base_model.model.layers.11.self_attn.v_proj.lora_A.default.weight', 'backbone.model.base_model.model.layers.11.self_attn.v_proj.lora_A.test.weight', 'backbone.model.base_model.model.layers.11.self_attn.v_proj.lora_B.default.weight', 'backbone.model.base_model.model.layers.11.self_attn.v_proj.lora_B.test.weight', 'backbone.model.base_model.model.layers.11.self_attn.o_proj.base_layer.weight', 'backbone.model.base_model.model.layers.11.self_attn.o_proj.lora_A.default.weight', 'backbone.model.base_model.model.layers.11.self_attn.o_proj.lora_A.test.weight', 'backbone.model.base_model.model.layers.11.self_attn.o_proj.lora_B.default.weight', 'backbone.model.base_model.model.layers.11.self_attn.o_proj.lora_B.test.weight', 'backbone.model.base_model.model.layers.11.mlp.gate_proj.base_layer.weight', 'backbone.model.base_model.model.layers.11.mlp.gate_proj.lora_A.default.weight', 'backbone.model.base_model.model.layers.11.mlp.gate_proj.lora_A.test.weight', 'backbone.model.base_model.model.layers.11.mlp.gate_proj.lora_B.default.weight', 'backbone.model.base_model.model.layers.11.mlp.gate_proj.lora_B.test.weight', 'backbone.model.base_model.model.layers.11.mlp.up_proj.base_layer.weight', 'backbone.model.base_model.model.layers.11.mlp.up_proj.lora_A.default.weight', 'backbone.model.base_model.model.layers.11.mlp.up_proj.lora_A.test.weight', 'backbone.model.base_model.model.layers.11.mlp.up_proj.lora_B.default.weight', 'backbone.model.base_model.model.layers.11.mlp.up_proj.lora_B.test.weight', 'backbone.model.base_model.model.layers.11.mlp.down_proj.base_layer.weight', 'backbone.model.base_model.model.layers.11.mlp.down_proj.lora_A.default.weight', 'backbone.model.base_model.model.layers.11.mlp.down_proj.lora_A.test.weight', 'backbone.model.base_model.model.layers.11.mlp.down_proj.lora_B.default.weight', 'backbone.model.base_model.model.layers.11.mlp.down_proj.lora_B.test.weight', 'backbone.model.base_model.model.layers.11.input_layernorm.weight', 'backbone.model.base_model.model.layers.11.post_attention_layernorm.weight', 'backbone.model.base_model.model.layers.12.self_attn.q_proj.base_layer.weight', 'backbone.model.base_model.model.layers.12.self_attn.q_proj.lora_A.default.weight', 'backbone.model.base_model.model.layers.12.self_attn.q_proj.lora_A.test.weight', 'backbone.model.base_model.model.layers.12.self_attn.q_proj.lora_B.default.weight', 'backbone.model.base_model.model.layers.12.self_attn.q_proj.lora_B.test.weight', 'backbone.model.base_model.model.layers.12.self_attn.k_proj.base_layer.weight', 'backbone.model.base_model.model.layers.12.self_attn.k_proj.lora_A.default.weight', 'backbone.model.base_model.model.layers.12.self_attn.k_proj.lora_A.test.weight', 'backbone.model.base_model.model.layers.12.self_attn.k_proj.lora_B.default.weight', 'backbone.model.base_model.model.layers.12.self_attn.k_proj.lora_B.test.weight', 'backbone.model.base_model.model.layers.12.self_attn.v_proj.base_layer.weight', 'backbone.model.base_model.model.layers.12.self_attn.v_proj.lora_A.default.weight', 'backbone.model.base_model.model.layers.12.self_attn.v_proj.lora_A.test.weight', 'backbone.model.base_model.model.layers.12.self_attn.v_proj.lora_B.default.weight', 'backbone.model.base_model.model.layers.12.self_attn.v_proj.lora_B.test.weight', 'backbone.model.base_model.model.layers.12.self_attn.o_proj.base_layer.weight', 'backbone.model.base_model.model.layers.12.self_attn.o_proj.lora_A.default.weight', 'backbone.model.base_model.model.layers.12.self_attn.o_proj.lora_A.test.weight', 'backbone.model.base_model.model.layers.12.self_attn.o_proj.lora_B.default.weight', 'backbone.model.base_model.model.layers.12.self_attn.o_proj.lora_B.test.weight', 'backbone.model.base_model.model.layers.12.mlp.gate_proj.base_layer.weight', 'backbone.model.base_model.model.layers.12.mlp.gate_proj.lora_A.default.weight', 'backbone.model.base_model.model.layers.12.mlp.gate_proj.lora_A.test.weight', 'backbone.model.base_model.model.layers.12.mlp.gate_proj.lora_B.default.weight', 'backbone.model.base_model.model.layers.12.mlp.gate_proj.lora_B.test.weight', 'backbone.model.base_model.model.layers.12.mlp.up_proj.base_layer.weight', 'backbone.model.base_model.model.layers.12.mlp.up_proj.lora_A.default.weight', 'backbone.model.base_model.model.layers.12.mlp.up_proj.lora_A.test.weight', 'backbone.model.base_model.model.layers.12.mlp.up_proj.lora_B.default.weight', 'backbone.model.base_model.model.layers.12.mlp.up_proj.lora_B.test.weight', 'backbone.model.base_model.model.layers.12.mlp.down_proj.base_layer.weight', 'backbone.model.base_model.model.layers.12.mlp.down_proj.lora_A.default.weight', 'backbone.model.base_model.model.layers.12.mlp.down_proj.lora_A.test.weight', 'backbone.model.base_model.model.layers.12.mlp.down_proj.lora_B.default.weight', 'backbone.model.base_model.model.layers.12.mlp.down_proj.lora_B.test.weight', 'backbone.model.base_model.model.layers.12.input_layernorm.weight', 'backbone.model.base_model.model.layers.12.post_attention_layernorm.weight', 'backbone.model.base_model.model.layers.13.self_attn.q_proj.base_layer.weight', 'backbone.model.base_model.model.layers.13.self_attn.q_proj.lora_A.default.weight', 'backbone.model.base_model.model.layers.13.self_attn.q_proj.lora_A.test.weight', 'backbone.model.base_model.model.layers.13.self_attn.q_proj.lora_B.default.weight', 'backbone.model.base_model.model.layers.13.self_attn.q_proj.lora_B.test.weight', 'backbone.model.base_model.model.layers.13.self_attn.k_proj.base_layer.weight', 'backbone.model.base_model.model.layers.13.self_attn.k_proj.lora_A.default.weight', 'backbone.model.base_model.model.layers.13.self_attn.k_proj.lora_A.test.weight', 'backbone.model.base_model.model.layers.13.self_attn.k_proj.lora_B.default.weight', 'backbone.model.base_model.model.layers.13.self_attn.k_proj.lora_B.test.weight', 'backbone.model.base_model.model.layers.13.self_attn.v_proj.base_layer.weight', 'backbone.model.base_model.model.layers.13.self_attn.v_proj.lora_A.default.weight', 'backbone.model.base_model.model.layers.13.self_attn.v_proj.lora_A.test.weight', 'backbone.model.base_model.model.layers.13.self_attn.v_proj.lora_B.default.weight', 'backbone.model.base_model.model.layers.13.self_attn.v_proj.lora_B.test.weight', 'backbone.model.base_model.model.layers.13.self_attn.o_proj.base_layer.weight', 'backbone.model.base_model.model.layers.13.self_attn.o_proj.lora_A.default.weight', 'backbone.model.base_model.model.layers.13.self_attn.o_proj.lora_A.test.weight', 'backbone.model.base_model.model.layers.13.self_attn.o_proj.lora_B.default.weight', 'backbone.model.base_model.model.layers.13.self_attn.o_proj.lora_B.test.weight', 'backbone.model.base_model.model.layers.13.mlp.gate_proj.base_layer.weight', 'backbone.model.base_model.model.layers.13.mlp.gate_proj.lora_A.default.weight', 'backbone.model.base_model.model.layers.13.mlp.gate_proj.lora_A.test.weight', 'backbone.model.base_model.model.layers.13.mlp.gate_proj.lora_B.default.weight', 'backbone.model.base_model.model.layers.13.mlp.gate_proj.lora_B.test.weight', 'backbone.model.base_model.model.layers.13.mlp.up_proj.base_layer.weight', 'backbone.model.base_model.model.layers.13.mlp.up_proj.lora_A.default.weight', 'backbone.model.base_model.model.layers.13.mlp.up_proj.lora_A.test.weight', 'backbone.model.base_model.model.layers.13.mlp.up_proj.lora_B.default.weight', 'backbone.model.base_model.model.layers.13.mlp.up_proj.lora_B.test.weight', 'backbone.model.base_model.model.layers.13.mlp.down_proj.base_layer.weight', 'backbone.model.base_model.model.layers.13.mlp.down_proj.lora_A.default.weight', 'backbone.model.base_model.model.layers.13.mlp.down_proj.lora_A.test.weight', 'backbone.model.base_model.model.layers.13.mlp.down_proj.lora_B.default.weight', 'backbone.model.base_model.model.layers.13.mlp.down_proj.lora_B.test.weight', 'backbone.model.base_model.model.layers.13.input_layernorm.weight', 'backbone.model.base_model.model.layers.13.post_attention_layernorm.weight', 'backbone.model.base_model.model.layers.14.self_attn.q_proj.base_layer.weight', 'backbone.model.base_model.model.layers.14.self_attn.q_proj.lora_A.default.weight', 'backbone.model.base_model.model.layers.14.self_attn.q_proj.lora_A.test.weight', 'backbone.model.base_model.model.layers.14.self_attn.q_proj.lora_B.default.weight', 'backbone.model.base_model.model.layers.14.self_attn.q_proj.lora_B.test.weight', 'backbone.model.base_model.model.layers.14.self_attn.k_proj.base_layer.weight', 'backbone.model.base_model.model.layers.14.self_attn.k_proj.lora_A.default.weight', 'backbone.model.base_model.model.layers.14.self_attn.k_proj.lora_A.test.weight', 'backbone.model.base_model.model.layers.14.self_attn.k_proj.lora_B.default.weight', 'backbone.model.base_model.model.layers.14.self_attn.k_proj.lora_B.test.weight', 'backbone.model.base_model.model.layers.14.self_attn.v_proj.base_layer.weight', 'backbone.model.base_model.model.layers.14.self_attn.v_proj.lora_A.default.weight', 'backbone.model.base_model.model.layers.14.self_attn.v_proj.lora_A.test.weight', 'backbone.model.base_model.model.layers.14.self_attn.v_proj.lora_B.default.weight', 'backbone.model.base_model.model.layers.14.self_attn.v_proj.lora_B.test.weight', 'backbone.model.base_model.model.layers.14.self_attn.o_proj.base_layer.weight', 'backbone.model.base_model.model.layers.14.self_attn.o_proj.lora_A.default.weight', 'backbone.model.base_model.model.layers.14.self_attn.o_proj.lora_A.test.weight', 'backbone.model.base_model.model.layers.14.self_attn.o_proj.lora_B.default.weight', 'backbone.model.base_model.model.layers.14.self_attn.o_proj.lora_B.test.weight', 'backbone.model.base_model.model.layers.14.mlp.gate_proj.base_layer.weight', 'backbone.model.base_model.model.layers.14.mlp.gate_proj.lora_A.default.weight', 'backbone.model.base_model.model.layers.14.mlp.gate_proj.lora_A.test.weight', 'backbone.model.base_model.model.layers.14.mlp.gate_proj.lora_B.default.weight', 'backbone.model.base_model.model.layers.14.mlp.gate_proj.lora_B.test.weight', 'backbone.model.base_model.model.layers.14.mlp.up_proj.base_layer.weight', 'backbone.model.base_model.model.layers.14.mlp.up_proj.lora_A.default.weight', 'backbone.model.base_model.model.layers.14.mlp.up_proj.lora_A.test.weight', 'backbone.model.base_model.model.layers.14.mlp.up_proj.lora_B.default.weight', 'backbone.model.base_model.model.layers.14.mlp.up_proj.lora_B.test.weight', 'backbone.model.base_model.model.layers.14.mlp.down_proj.base_layer.weight', 'backbone.model.base_model.model.layers.14.mlp.down_proj.lora_A.default.weight', 'backbone.model.base_model.model.layers.14.mlp.down_proj.lora_A.test.weight', 'backbone.model.base_model.model.layers.14.mlp.down_proj.lora_B.default.weight', 'backbone.model.base_model.model.layers.14.mlp.down_proj.lora_B.test.weight', 'backbone.model.base_model.model.layers.14.input_layernorm.weight', 'backbone.model.base_model.model.layers.14.post_attention_layernorm.weight', 'backbone.model.base_model.model.layers.15.self_attn.q_proj.base_layer.weight', 'backbone.model.base_model.model.layers.15.self_attn.q_proj.lora_A.default.weight', 'backbone.model.base_model.model.layers.15.self_attn.q_proj.lora_A.test.weight', 'backbone.model.base_model.model.layers.15.self_attn.q_proj.lora_B.default.weight', 'backbone.model.base_model.model.layers.15.self_attn.q_proj.lora_B.test.weight', 'backbone.model.base_model.model.layers.15.self_attn.k_proj.base_layer.weight', 'backbone.model.base_model.model.layers.15.self_attn.k_proj.lora_A.default.weight', 'backbone.model.base_model.model.layers.15.self_attn.k_proj.lora_A.test.weight', 'backbone.model.base_model.model.layers.15.self_attn.k_proj.lora_B.default.weight', 'backbone.model.base_model.model.layers.15.self_attn.k_proj.lora_B.test.weight', 'backbone.model.base_model.model.layers.15.self_attn.v_proj.base_layer.weight', 'backbone.model.base_model.model.layers.15.self_attn.v_proj.lora_A.default.weight', 'backbone.model.base_model.model.layers.15.self_attn.v_proj.lora_A.test.weight', 'backbone.model.base_model.model.layers.15.self_attn.v_proj.lora_B.default.weight', 'backbone.model.base_model.model.layers.15.self_attn.v_proj.lora_B.test.weight', 'backbone.model.base_model.model.layers.15.self_attn.o_proj.base_layer.weight', 'backbone.model.base_model.model.layers.15.self_attn.o_proj.lora_A.default.weight', 'backbone.model.base_model.model.layers.15.self_attn.o_proj.lora_A.test.weight', 'backbone.model.base_model.model.layers.15.self_attn.o_proj.lora_B.default.weight', 'backbone.model.base_model.model.layers.15.self_attn.o_proj.lora_B.test.weight', 'backbone.model.base_model.model.layers.15.mlp.gate_proj.base_layer.weight', 'backbone.model.base_model.model.layers.15.mlp.gate_proj.lora_A.default.weight', 'backbone.model.base_model.model.layers.15.mlp.gate_proj.lora_A.test.weight', 'backbone.model.base_model.model.layers.15.mlp.gate_proj.lora_B.default.weight', 'backbone.model.base_model.model.layers.15.mlp.gate_proj.lora_B.test.weight', 'backbone.model.base_model.model.layers.15.mlp.up_proj.base_layer.weight', 'backbone.model.base_model.model.layers.15.mlp.up_proj.lora_A.default.weight', 'backbone.model.base_model.model.layers.15.mlp.up_proj.lora_A.test.weight', 'backbone.model.base_model.model.layers.15.mlp.up_proj.lora_B.default.weight', 'backbone.model.base_model.model.layers.15.mlp.up_proj.lora_B.test.weight', 'backbone.model.base_model.model.layers.15.mlp.down_proj.base_layer.weight', 'backbone.model.base_model.model.layers.15.mlp.down_proj.lora_A.default.weight', 'backbone.model.base_model.model.layers.15.mlp.down_proj.lora_A.test.weight', 'backbone.model.base_model.model.layers.15.mlp.down_proj.lora_B.default.weight', 'backbone.model.base_model.model.layers.15.mlp.down_proj.lora_B.test.weight', 'backbone.model.base_model.model.layers.15.input_layernorm.weight', 'backbone.model.base_model.model.layers.15.post_attention_layernorm.weight', 'backbone.model.base_model.model.layers.16.self_attn.q_proj.base_layer.weight', 'backbone.model.base_model.model.layers.16.self_attn.q_proj.lora_A.default.weight', 'backbone.model.base_model.model.layers.16.self_attn.q_proj.lora_A.test.weight', 'backbone.model.base_model.model.layers.16.self_attn.q_proj.lora_B.default.weight', 'backbone.model.base_model.model.layers.16.self_attn.q_proj.lora_B.test.weight', 'backbone.model.base_model.model.layers.16.self_attn.k_proj.base_layer.weight', 'backbone.model.base_model.model.layers.16.self_attn.k_proj.lora_A.default.weight', 'backbone.model.base_model.model.layers.16.self_attn.k_proj.lora_A.test.weight', 'backbone.model.base_model.model.layers.16.self_attn.k_proj.lora_B.default.weight', 'backbone.model.base_model.model.layers.16.self_attn.k_proj.lora_B.test.weight', 'backbone.model.base_model.model.layers.16.self_attn.v_proj.base_layer.weight', 'backbone.model.base_model.model.layers.16.self_attn.v_proj.lora_A.default.weight', 'backbone.model.base_model.model.layers.16.self_attn.v_proj.lora_A.test.weight', 'backbone.model.base_model.model.layers.16.self_attn.v_proj.lora_B.default.weight', 'backbone.model.base_model.model.layers.16.self_attn.v_proj.lora_B.test.weight', 'backbone.model.base_model.model.layers.16.self_attn.o_proj.base_layer.weight', 'backbone.model.base_model.model.layers.16.self_attn.o_proj.lora_A.default.weight', 'backbone.model.base_model.model.layers.16.self_attn.o_proj.lora_A.test.weight', 'backbone.model.base_model.model.layers.16.self_attn.o_proj.lora_B.default.weight', 'backbone.model.base_model.model.layers.16.self_attn.o_proj.lora_B.test.weight', 'backbone.model.base_model.model.layers.16.mlp.gate_proj.base_layer.weight', 'backbone.model.base_model.model.layers.16.mlp.gate_proj.lora_A.default.weight', 'backbone.model.base_model.model.layers.16.mlp.gate_proj.lora_A.test.weight', 'backbone.model.base_model.model.layers.16.mlp.gate_proj.lora_B.default.weight', 'backbone.model.base_model.model.layers.16.mlp.gate_proj.lora_B.test.weight', 'backbone.model.base_model.model.layers.16.mlp.up_proj.base_layer.weight', 'backbone.model.base_model.model.layers.16.mlp.up_proj.lora_A.default.weight', 'backbone.model.base_model.model.layers.16.mlp.up_proj.lora_A.test.weight', 'backbone.model.base_model.model.layers.16.mlp.up_proj.lora_B.default.weight', 'backbone.model.base_model.model.layers.16.mlp.up_proj.lora_B.test.weight', 'backbone.model.base_model.model.layers.16.mlp.down_proj.base_layer.weight', 'backbone.model.base_model.model.layers.16.mlp.down_proj.lora_A.default.weight', 'backbone.model.base_model.model.layers.16.mlp.down_proj.lora_A.test.weight', 'backbone.model.base_model.model.layers.16.mlp.down_proj.lora_B.default.weight', 'backbone.model.base_model.model.layers.16.mlp.down_proj.lora_B.test.weight', 'backbone.model.base_model.model.layers.16.input_layernorm.weight', 'backbone.model.base_model.model.layers.16.post_attention_layernorm.weight', 'backbone.model.base_model.model.layers.17.self_attn.q_proj.base_layer.weight', 'backbone.model.base_model.model.layers.17.self_attn.q_proj.lora_A.default.weight', 'backbone.model.base_model.model.layers.17.self_attn.q_proj.lora_A.test.weight', 'backbone.model.base_model.model.layers.17.self_attn.q_proj.lora_B.default.weight', 'backbone.model.base_model.model.layers.17.self_attn.q_proj.lora_B.test.weight', 'backbone.model.base_model.model.layers.17.self_attn.k_proj.base_layer.weight', 'backbone.model.base_model.model.layers.17.self_attn.k_proj.lora_A.default.weight', 'backbone.model.base_model.model.layers.17.self_attn.k_proj.lora_A.test.weight', 'backbone.model.base_model.model.layers.17.self_attn.k_proj.lora_B.default.weight', 'backbone.model.base_model.model.layers.17.self_attn.k_proj.lora_B.test.weight', 'backbone.model.base_model.model.layers.17.self_attn.v_proj.base_layer.weight', 'backbone.model.base_model.model.layers.17.self_attn.v_proj.lora_A.default.weight', 'backbone.model.base_model.model.layers.17.self_attn.v_proj.lora_A.test.weight', 'backbone.model.base_model.model.layers.17.self_attn.v_proj.lora_B.default.weight', 'backbone.model.base_model.model.layers.17.self_attn.v_proj.lora_B.test.weight', 'backbone.model.base_model.model.layers.17.self_attn.o_proj.base_layer.weight', 'backbone.model.base_model.model.layers.17.self_attn.o_proj.lora_A.default.weight', 'backbone.model.base_model.model.layers.17.self_attn.o_proj.lora_A.test.weight', 'backbone.model.base_model.model.layers.17.self_attn.o_proj.lora_B.default.weight', 'backbone.model.base_model.model.layers.17.self_attn.o_proj.lora_B.test.weight', 'backbone.model.base_model.model.layers.17.mlp.gate_proj.base_layer.weight', 'backbone.model.base_model.model.layers.17.mlp.gate_proj.lora_A.default.weight', 'backbone.model.base_model.model.layers.17.mlp.gate_proj.lora_A.test.weight', 'backbone.model.base_model.model.layers.17.mlp.gate_proj.lora_B.default.weight', 'backbone.model.base_model.model.layers.17.mlp.gate_proj.lora_B.test.weight', 'backbone.model.base_model.model.layers.17.mlp.up_proj.base_layer.weight', 'backbone.model.base_model.model.layers.17.mlp.up_proj.lora_A.default.weight', 'backbone.model.base_model.model.layers.17.mlp.up_proj.lora_A.test.weight', 'backbone.model.base_model.model.layers.17.mlp.up_proj.lora_B.default.weight', 'backbone.model.base_model.model.layers.17.mlp.up_proj.lora_B.test.weight', 'backbone.model.base_model.model.layers.17.mlp.down_proj.base_layer.weight', 'backbone.model.base_model.model.layers.17.mlp.down_proj.lora_A.default.weight', 'backbone.model.base_model.model.layers.17.mlp.down_proj.lora_A.test.weight', 'backbone.model.base_model.model.layers.17.mlp.down_proj.lora_B.default.weight', 'backbone.model.base_model.model.layers.17.mlp.down_proj.lora_B.test.weight', 'backbone.model.base_model.model.layers.17.input_layernorm.weight', 'backbone.model.base_model.model.layers.17.post_attention_layernorm.weight', 'backbone.model.base_model.model.layers.18.self_attn.q_proj.base_layer.weight', 'backbone.model.base_model.model.layers.18.self_attn.q_proj.lora_A.default.weight', 'backbone.model.base_model.model.layers.18.self_attn.q_proj.lora_A.test.weight', 'backbone.model.base_model.model.layers.18.self_attn.q_proj.lora_B.default.weight', 'backbone.model.base_model.model.layers.18.self_attn.q_proj.lora_B.test.weight', 'backbone.model.base_model.model.layers.18.self_attn.k_proj.base_layer.weight', 'backbone.model.base_model.model.layers.18.self_attn.k_proj.lora_A.default.weight', 'backbone.model.base_model.model.layers.18.self_attn.k_proj.lora_A.test.weight', 'backbone.model.base_model.model.layers.18.self_attn.k_proj.lora_B.default.weight', 'backbone.model.base_model.model.layers.18.self_attn.k_proj.lora_B.test.weight', 'backbone.model.base_model.model.layers.18.self_attn.v_proj.base_layer.weight', 'backbone.model.base_model.model.layers.18.self_attn.v_proj.lora_A.default.weight', 'backbone.model.base_model.model.layers.18.self_attn.v_proj.lora_A.test.weight', 'backbone.model.base_model.model.layers.18.self_attn.v_proj.lora_B.default.weight', 'backbone.model.base_model.model.layers.18.self_attn.v_proj.lora_B.test.weight', 'backbone.model.base_model.model.layers.18.self_attn.o_proj.base_layer.weight', 'backbone.model.base_model.model.layers.18.self_attn.o_proj.lora_A.default.weight', 'backbone.model.base_model.model.layers.18.self_attn.o_proj.lora_A.test.weight', 'backbone.model.base_model.model.layers.18.self_attn.o_proj.lora_B.default.weight', 'backbone.model.base_model.model.layers.18.self_attn.o_proj.lora_B.test.weight', 'backbone.model.base_model.model.layers.18.mlp.gate_proj.base_layer.weight', 'backbone.model.base_model.model.layers.18.mlp.gate_proj.lora_A.default.weight', 'backbone.model.base_model.model.layers.18.mlp.gate_proj.lora_A.test.weight', 'backbone.model.base_model.model.layers.18.mlp.gate_proj.lora_B.default.weight', 'backbone.model.base_model.model.layers.18.mlp.gate_proj.lora_B.test.weight', 'backbone.model.base_model.model.layers.18.mlp.up_proj.base_layer.weight', 'backbone.model.base_model.model.layers.18.mlp.up_proj.lora_A.default.weight', 'backbone.model.base_model.model.layers.18.mlp.up_proj.lora_A.test.weight', 'backbone.model.base_model.model.layers.18.mlp.up_proj.lora_B.default.weight', 'backbone.model.base_model.model.layers.18.mlp.up_proj.lora_B.test.weight', 'backbone.model.base_model.model.layers.18.mlp.down_proj.base_layer.weight', 'backbone.model.base_model.model.layers.18.mlp.down_proj.lora_A.default.weight', 'backbone.model.base_model.model.layers.18.mlp.down_proj.lora_A.test.weight', 'backbone.model.base_model.model.layers.18.mlp.down_proj.lora_B.default.weight', 'backbone.model.base_model.model.layers.18.mlp.down_proj.lora_B.test.weight', 'backbone.model.base_model.model.layers.18.input_layernorm.weight', 'backbone.model.base_model.model.layers.18.post_attention_layernorm.weight', 'backbone.model.base_model.model.layers.19.self_attn.q_proj.base_layer.weight', 'backbone.model.base_model.model.layers.19.self_attn.q_proj.lora_A.default.weight', 'backbone.model.base_model.model.layers.19.self_attn.q_proj.lora_A.test.weight', 'backbone.model.base_model.model.layers.19.self_attn.q_proj.lora_B.default.weight', 'backbone.model.base_model.model.layers.19.self_attn.q_proj.lora_B.test.weight', 'backbone.model.base_model.model.layers.19.self_attn.k_proj.base_layer.weight', 'backbone.model.base_model.model.layers.19.self_attn.k_proj.lora_A.default.weight', 'backbone.model.base_model.model.layers.19.self_attn.k_proj.lora_A.test.weight', 'backbone.model.base_model.model.layers.19.self_attn.k_proj.lora_B.default.weight', 'backbone.model.base_model.model.layers.19.self_attn.k_proj.lora_B.test.weight', 'backbone.model.base_model.model.layers.19.self_attn.v_proj.base_layer.weight', 'backbone.model.base_model.model.layers.19.self_attn.v_proj.lora_A.default.weight', 'backbone.model.base_model.model.layers.19.self_attn.v_proj.lora_A.test.weight', 'backbone.model.base_model.model.layers.19.self_attn.v_proj.lora_B.default.weight', 'backbone.model.base_model.model.layers.19.self_attn.v_proj.lora_B.test.weight', 'backbone.model.base_model.model.layers.19.self_attn.o_proj.base_layer.weight', 'backbone.model.base_model.model.layers.19.self_attn.o_proj.lora_A.default.weight', 'backbone.model.base_model.model.layers.19.self_attn.o_proj.lora_A.test.weight', 'backbone.model.base_model.model.layers.19.self_attn.o_proj.lora_B.default.weight', 'backbone.model.base_model.model.layers.19.self_attn.o_proj.lora_B.test.weight', 'backbone.model.base_model.model.layers.19.mlp.gate_proj.base_layer.weight', 'backbone.model.base_model.model.layers.19.mlp.gate_proj.lora_A.default.weight', 'backbone.model.base_model.model.layers.19.mlp.gate_proj.lora_A.test.weight', 'backbone.model.base_model.model.layers.19.mlp.gate_proj.lora_B.default.weight', 'backbone.model.base_model.model.layers.19.mlp.gate_proj.lora_B.test.weight', 'backbone.model.base_model.model.layers.19.mlp.up_proj.base_layer.weight', 'backbone.model.base_model.model.layers.19.mlp.up_proj.lora_A.default.weight', 'backbone.model.base_model.model.layers.19.mlp.up_proj.lora_A.test.weight', 'backbone.model.base_model.model.layers.19.mlp.up_proj.lora_B.default.weight', 'backbone.model.base_model.model.layers.19.mlp.up_proj.lora_B.test.weight', 'backbone.model.base_model.model.layers.19.mlp.down_proj.base_layer.weight', 'backbone.model.base_model.model.layers.19.mlp.down_proj.lora_A.default.weight', 'backbone.model.base_model.model.layers.19.mlp.down_proj.lora_A.test.weight', 'backbone.model.base_model.model.layers.19.mlp.down_proj.lora_B.default.weight', 'backbone.model.base_model.model.layers.19.mlp.down_proj.lora_B.test.weight', 'backbone.model.base_model.model.layers.19.input_layernorm.weight', 'backbone.model.base_model.model.layers.19.post_attention_layernorm.weight', 'backbone.model.base_model.model.layers.20.self_attn.q_proj.base_layer.weight', 'backbone.model.base_model.model.layers.20.self_attn.q_proj.lora_A.default.weight', 'backbone.model.base_model.model.layers.20.self_attn.q_proj.lora_A.test.weight', 'backbone.model.base_model.model.layers.20.self_attn.q_proj.lora_B.default.weight', 'backbone.model.base_model.model.layers.20.self_attn.q_proj.lora_B.test.weight', 'backbone.model.base_model.model.layers.20.self_attn.k_proj.base_layer.weight', 'backbone.model.base_model.model.layers.20.self_attn.k_proj.lora_A.default.weight', 'backbone.model.base_model.model.layers.20.self_attn.k_proj.lora_A.test.weight', 'backbone.model.base_model.model.layers.20.self_attn.k_proj.lora_B.default.weight', 'backbone.model.base_model.model.layers.20.self_attn.k_proj.lora_B.test.weight', 'backbone.model.base_model.model.layers.20.self_attn.v_proj.base_layer.weight', 'backbone.model.base_model.model.layers.20.self_attn.v_proj.lora_A.default.weight', 'backbone.model.base_model.model.layers.20.self_attn.v_proj.lora_A.test.weight', 'backbone.model.base_model.model.layers.20.self_attn.v_proj.lora_B.default.weight', 'backbone.model.base_model.model.layers.20.self_attn.v_proj.lora_B.test.weight', 'backbone.model.base_model.model.layers.20.self_attn.o_proj.base_layer.weight', 'backbone.model.base_model.model.layers.20.self_attn.o_proj.lora_A.default.weight', 'backbone.model.base_model.model.layers.20.self_attn.o_proj.lora_A.test.weight', 'backbone.model.base_model.model.layers.20.self_attn.o_proj.lora_B.default.weight', 'backbone.model.base_model.model.layers.20.self_attn.o_proj.lora_B.test.weight', 'backbone.model.base_model.model.layers.20.mlp.gate_proj.base_layer.weight', 'backbone.model.base_model.model.layers.20.mlp.gate_proj.lora_A.default.weight', 'backbone.model.base_model.model.layers.20.mlp.gate_proj.lora_A.test.weight', 'backbone.model.base_model.model.layers.20.mlp.gate_proj.lora_B.default.weight', 'backbone.model.base_model.model.layers.20.mlp.gate_proj.lora_B.test.weight', 'backbone.model.base_model.model.layers.20.mlp.up_proj.base_layer.weight', 'backbone.model.base_model.model.layers.20.mlp.up_proj.lora_A.default.weight', 'backbone.model.base_model.model.layers.20.mlp.up_proj.lora_A.test.weight', 'backbone.model.base_model.model.layers.20.mlp.up_proj.lora_B.default.weight', 'backbone.model.base_model.model.layers.20.mlp.up_proj.lora_B.test.weight', 'backbone.model.base_model.model.layers.20.mlp.down_proj.base_layer.weight', 'backbone.model.base_model.model.layers.20.mlp.down_proj.lora_A.default.weight', 'backbone.model.base_model.model.layers.20.mlp.down_proj.lora_A.test.weight', 'backbone.model.base_model.model.layers.20.mlp.down_proj.lora_B.default.weight', 'backbone.model.base_model.model.layers.20.mlp.down_proj.lora_B.test.weight', 'backbone.model.base_model.model.layers.20.input_layernorm.weight', 'backbone.model.base_model.model.layers.20.post_attention_layernorm.weight', 'backbone.model.base_model.model.layers.21.self_attn.q_proj.base_layer.weight', 'backbone.model.base_model.model.layers.21.self_attn.q_proj.lora_A.default.weight', 'backbone.model.base_model.model.layers.21.self_attn.q_proj.lora_A.test.weight', 'backbone.model.base_model.model.layers.21.self_attn.q_proj.lora_B.default.weight', 'backbone.model.base_model.model.layers.21.self_attn.q_proj.lora_B.test.weight', 'backbone.model.base_model.model.layers.21.self_attn.k_proj.base_layer.weight', 'backbone.model.base_model.model.layers.21.self_attn.k_proj.lora_A.default.weight', 'backbone.model.base_model.model.layers.21.self_attn.k_proj.lora_A.test.weight', 'backbone.model.base_model.model.layers.21.self_attn.k_proj.lora_B.default.weight', 'backbone.model.base_model.model.layers.21.self_attn.k_proj.lora_B.test.weight', 'backbone.model.base_model.model.layers.21.self_attn.v_proj.base_layer.weight', 'backbone.model.base_model.model.layers.21.self_attn.v_proj.lora_A.default.weight', 'backbone.model.base_model.model.layers.21.self_attn.v_proj.lora_A.test.weight', 'backbone.model.base_model.model.layers.21.self_attn.v_proj.lora_B.default.weight', 'backbone.model.base_model.model.layers.21.self_attn.v_proj.lora_B.test.weight', 'backbone.model.base_model.model.layers.21.self_attn.o_proj.base_layer.weight', 'backbone.model.base_model.model.layers.21.self_attn.o_proj.lora_A.default.weight', 'backbone.model.base_model.model.layers.21.self_attn.o_proj.lora_A.test.weight', 'backbone.model.base_model.model.layers.21.self_attn.o_proj.lora_B.default.weight', 'backbone.model.base_model.model.layers.21.self_attn.o_proj.lora_B.test.weight', 'backbone.model.base_model.model.layers.21.mlp.gate_proj.base_layer.weight', 'backbone.model.base_model.model.layers.21.mlp.gate_proj.lora_A.default.weight', 'backbone.model.base_model.model.layers.21.mlp.gate_proj.lora_A.test.weight', 'backbone.model.base_model.model.layers.21.mlp.gate_proj.lora_B.default.weight', 'backbone.model.base_model.model.layers.21.mlp.gate_proj.lora_B.test.weight', 'backbone.model.base_model.model.layers.21.mlp.up_proj.base_layer.weight', 'backbone.model.base_model.model.layers.21.mlp.up_proj.lora_A.default.weight', 'backbone.model.base_model.model.layers.21.mlp.up_proj.lora_A.test.weight', 'backbone.model.base_model.model.layers.21.mlp.up_proj.lora_B.default.weight', 'backbone.model.base_model.model.layers.21.mlp.up_proj.lora_B.test.weight', 'backbone.model.base_model.model.layers.21.mlp.down_proj.base_layer.weight', 'backbone.model.base_model.model.layers.21.mlp.down_proj.lora_A.default.weight', 'backbone.model.base_model.model.layers.21.mlp.down_proj.lora_A.test.weight', 'backbone.model.base_model.model.layers.21.mlp.down_proj.lora_B.default.weight', 'backbone.model.base_model.model.layers.21.mlp.down_proj.lora_B.test.weight', 'backbone.model.base_model.model.layers.21.input_layernorm.weight', 'backbone.model.base_model.model.layers.21.post_attention_layernorm.weight', 'backbone.model.base_model.model.layers.22.self_attn.q_proj.base_layer.weight', 'backbone.model.base_model.model.layers.22.self_attn.q_proj.lora_A.default.weight', 'backbone.model.base_model.model.layers.22.self_attn.q_proj.lora_A.test.weight', 'backbone.model.base_model.model.layers.22.self_attn.q_proj.lora_B.default.weight', 'backbone.model.base_model.model.layers.22.self_attn.q_proj.lora_B.test.weight', 'backbone.model.base_model.model.layers.22.self_attn.k_proj.base_layer.weight', 'backbone.model.base_model.model.layers.22.self_attn.k_proj.lora_A.default.weight', 'backbone.model.base_model.model.layers.22.self_attn.k_proj.lora_A.test.weight', 'backbone.model.base_model.model.layers.22.self_attn.k_proj.lora_B.default.weight', 'backbone.model.base_model.model.layers.22.self_attn.k_proj.lora_B.test.weight', 'backbone.model.base_model.model.layers.22.self_attn.v_proj.base_layer.weight', 'backbone.model.base_model.model.layers.22.self_attn.v_proj.lora_A.default.weight', 'backbone.model.base_model.model.layers.22.self_attn.v_proj.lora_A.test.weight', 'backbone.model.base_model.model.layers.22.self_attn.v_proj.lora_B.default.weight', 'backbone.model.base_model.model.layers.22.self_attn.v_proj.lora_B.test.weight', 'backbone.model.base_model.model.layers.22.self_attn.o_proj.base_layer.weight', 'backbone.model.base_model.model.layers.22.self_attn.o_proj.lora_A.default.weight', 'backbone.model.base_model.model.layers.22.self_attn.o_proj.lora_A.test.weight', 'backbone.model.base_model.model.layers.22.self_attn.o_proj.lora_B.default.weight', 'backbone.model.base_model.model.layers.22.self_attn.o_proj.lora_B.test.weight', 'backbone.model.base_model.model.layers.22.mlp.gate_proj.base_layer.weight', 'backbone.model.base_model.model.layers.22.mlp.gate_proj.lora_A.default.weight', 'backbone.model.base_model.model.layers.22.mlp.gate_proj.lora_A.test.weight', 'backbone.model.base_model.model.layers.22.mlp.gate_proj.lora_B.default.weight', 'backbone.model.base_model.model.layers.22.mlp.gate_proj.lora_B.test.weight', 'backbone.model.base_model.model.layers.22.mlp.up_proj.base_layer.weight', 'backbone.model.base_model.model.layers.22.mlp.up_proj.lora_A.default.weight', 'backbone.model.base_model.model.layers.22.mlp.up_proj.lora_A.test.weight', 'backbone.model.base_model.model.layers.22.mlp.up_proj.lora_B.default.weight', 'backbone.model.base_model.model.layers.22.mlp.up_proj.lora_B.test.weight', 'backbone.model.base_model.model.layers.22.mlp.down_proj.base_layer.weight', 'backbone.model.base_model.model.layers.22.mlp.down_proj.lora_A.default.weight', 'backbone.model.base_model.model.layers.22.mlp.down_proj.lora_A.test.weight', 'backbone.model.base_model.model.layers.22.mlp.down_proj.lora_B.default.weight', 'backbone.model.base_model.model.layers.22.mlp.down_proj.lora_B.test.weight', 'backbone.model.base_model.model.layers.22.input_layernorm.weight', 'backbone.model.base_model.model.layers.22.post_attention_layernorm.weight', 'backbone.model.base_model.model.layers.23.self_attn.q_proj.base_layer.weight', 'backbone.model.base_model.model.layers.23.self_attn.q_proj.lora_A.default.weight', 'backbone.model.base_model.model.layers.23.self_attn.q_proj.lora_A.test.weight', 'backbone.model.base_model.model.layers.23.self_attn.q_proj.lora_B.default.weight', 'backbone.model.base_model.model.layers.23.self_attn.q_proj.lora_B.test.weight', 'backbone.model.base_model.model.layers.23.self_attn.k_proj.base_layer.weight', 'backbone.model.base_model.model.layers.23.self_attn.k_proj.lora_A.default.weight', 'backbone.model.base_model.model.layers.23.self_attn.k_proj.lora_A.test.weight', 'backbone.model.base_model.model.layers.23.self_attn.k_proj.lora_B.default.weight', 'backbone.model.base_model.model.layers.23.self_attn.k_proj.lora_B.test.weight', 'backbone.model.base_model.model.layers.23.self_attn.v_proj.base_layer.weight', 'backbone.model.base_model.model.layers.23.self_attn.v_proj.lora_A.default.weight', 'backbone.model.base_model.model.layers.23.self_attn.v_proj.lora_A.test.weight', 'backbone.model.base_model.model.layers.23.self_attn.v_proj.lora_B.default.weight', 'backbone.model.base_model.model.layers.23.self_attn.v_proj.lora_B.test.weight', 'backbone.model.base_model.model.layers.23.self_attn.o_proj.base_layer.weight', 'backbone.model.base_model.model.layers.23.self_attn.o_proj.lora_A.default.weight', 'backbone.model.base_model.model.layers.23.self_attn.o_proj.lora_A.test.weight', 'backbone.model.base_model.model.layers.23.self_attn.o_proj.lora_B.default.weight', 'backbone.model.base_model.model.layers.23.self_attn.o_proj.lora_B.test.weight', 'backbone.model.base_model.model.layers.23.mlp.gate_proj.base_layer.weight', 'backbone.model.base_model.model.layers.23.mlp.gate_proj.lora_A.default.weight', 'backbone.model.base_model.model.layers.23.mlp.gate_proj.lora_A.test.weight', 'backbone.model.base_model.model.layers.23.mlp.gate_proj.lora_B.default.weight', 'backbone.model.base_model.model.layers.23.mlp.gate_proj.lora_B.test.weight', 'backbone.model.base_model.model.layers.23.mlp.up_proj.base_layer.weight', 'backbone.model.base_model.model.layers.23.mlp.up_proj.lora_A.default.weight', 'backbone.model.base_model.model.layers.23.mlp.up_proj.lora_A.test.weight', 'backbone.model.base_model.model.layers.23.mlp.up_proj.lora_B.default.weight', 'backbone.model.base_model.model.layers.23.mlp.up_proj.lora_B.test.weight', 'backbone.model.base_model.model.layers.23.mlp.down_proj.base_layer.weight', 'backbone.model.base_model.model.layers.23.mlp.down_proj.lora_A.default.weight', 'backbone.model.base_model.model.layers.23.mlp.down_proj.lora_A.test.weight', 'backbone.model.base_model.model.layers.23.mlp.down_proj.lora_B.default.weight', 'backbone.model.base_model.model.layers.23.mlp.down_proj.lora_B.test.weight', 'backbone.model.base_model.model.layers.23.input_layernorm.weight', 'backbone.model.base_model.model.layers.23.post_attention_layernorm.weight', 'backbone.model.base_model.model.layers.24.self_attn.q_proj.base_layer.weight', 'backbone.model.base_model.model.layers.24.self_attn.q_proj.lora_A.default.weight', 'backbone.model.base_model.model.layers.24.self_attn.q_proj.lora_A.test.weight', 'backbone.model.base_model.model.layers.24.self_attn.q_proj.lora_B.default.weight', 'backbone.model.base_model.model.layers.24.self_attn.q_proj.lora_B.test.weight', 'backbone.model.base_model.model.layers.24.self_attn.k_proj.base_layer.weight', 'backbone.model.base_model.model.layers.24.self_attn.k_proj.lora_A.default.weight', 'backbone.model.base_model.model.layers.24.self_attn.k_proj.lora_A.test.weight', 'backbone.model.base_model.model.layers.24.self_attn.k_proj.lora_B.default.weight', 'backbone.model.base_model.model.layers.24.self_attn.k_proj.lora_B.test.weight', 'backbone.model.base_model.model.layers.24.self_attn.v_proj.base_layer.weight', 'backbone.model.base_model.model.layers.24.self_attn.v_proj.lora_A.default.weight', 'backbone.model.base_model.model.layers.24.self_attn.v_proj.lora_A.test.weight', 'backbone.model.base_model.model.layers.24.self_attn.v_proj.lora_B.default.weight', 'backbone.model.base_model.model.layers.24.self_attn.v_proj.lora_B.test.weight', 'backbone.model.base_model.model.layers.24.self_attn.o_proj.base_layer.weight', 'backbone.model.base_model.model.layers.24.self_attn.o_proj.lora_A.default.weight', 'backbone.model.base_model.model.layers.24.self_attn.o_proj.lora_A.test.weight', 'backbone.model.base_model.model.layers.24.self_attn.o_proj.lora_B.default.weight', 'backbone.model.base_model.model.layers.24.self_attn.o_proj.lora_B.test.weight', 'backbone.model.base_model.model.layers.24.mlp.gate_proj.base_layer.weight', 'backbone.model.base_model.model.layers.24.mlp.gate_proj.lora_A.default.weight', 'backbone.model.base_model.model.layers.24.mlp.gate_proj.lora_A.test.weight', 'backbone.model.base_model.model.layers.24.mlp.gate_proj.lora_B.default.weight', 'backbone.model.base_model.model.layers.24.mlp.gate_proj.lora_B.test.weight', 'backbone.model.base_model.model.layers.24.mlp.up_proj.base_layer.weight', 'backbone.model.base_model.model.layers.24.mlp.up_proj.lora_A.default.weight', 'backbone.model.base_model.model.layers.24.mlp.up_proj.lora_A.test.weight', 'backbone.model.base_model.model.layers.24.mlp.up_proj.lora_B.default.weight', 'backbone.model.base_model.model.layers.24.mlp.up_proj.lora_B.test.weight', 'backbone.model.base_model.model.layers.24.mlp.down_proj.base_layer.weight', 'backbone.model.base_model.model.layers.24.mlp.down_proj.lora_A.default.weight', 'backbone.model.base_model.model.layers.24.mlp.down_proj.lora_A.test.weight', 'backbone.model.base_model.model.layers.24.mlp.down_proj.lora_B.default.weight', 'backbone.model.base_model.model.layers.24.mlp.down_proj.lora_B.test.weight', 'backbone.model.base_model.model.layers.24.input_layernorm.weight', 'backbone.model.base_model.model.layers.24.post_attention_layernorm.weight', 'backbone.model.base_model.model.layers.25.self_attn.q_proj.base_layer.weight', 'backbone.model.base_model.model.layers.25.self_attn.q_proj.lora_A.default.weight', 'backbone.model.base_model.model.layers.25.self_attn.q_proj.lora_A.test.weight', 'backbone.model.base_model.model.layers.25.self_attn.q_proj.lora_B.default.weight', 'backbone.model.base_model.model.layers.25.self_attn.q_proj.lora_B.test.weight', 'backbone.model.base_model.model.layers.25.self_attn.k_proj.base_layer.weight', 'backbone.model.base_model.model.layers.25.self_attn.k_proj.lora_A.default.weight', 'backbone.model.base_model.model.layers.25.self_attn.k_proj.lora_A.test.weight', 'backbone.model.base_model.model.layers.25.self_attn.k_proj.lora_B.default.weight', 'backbone.model.base_model.model.layers.25.self_attn.k_proj.lora_B.test.weight', 'backbone.model.base_model.model.layers.25.self_attn.v_proj.base_layer.weight', 'backbone.model.base_model.model.layers.25.self_attn.v_proj.lora_A.default.weight', 'backbone.model.base_model.model.layers.25.self_attn.v_proj.lora_A.test.weight', 'backbone.model.base_model.model.layers.25.self_attn.v_proj.lora_B.default.weight', 'backbone.model.base_model.model.layers.25.self_attn.v_proj.lora_B.test.weight', 'backbone.model.base_model.model.layers.25.self_attn.o_proj.base_layer.weight', 'backbone.model.base_model.model.layers.25.self_attn.o_proj.lora_A.default.weight', 'backbone.model.base_model.model.layers.25.self_attn.o_proj.lora_A.test.weight', 'backbone.model.base_model.model.layers.25.self_attn.o_proj.lora_B.default.weight', 'backbone.model.base_model.model.layers.25.self_attn.o_proj.lora_B.test.weight', 'backbone.model.base_model.model.layers.25.mlp.gate_proj.base_layer.weight', 'backbone.model.base_model.model.layers.25.mlp.gate_proj.lora_A.default.weight', 'backbone.model.base_model.model.layers.25.mlp.gate_proj.lora_A.test.weight', 'backbone.model.base_model.model.layers.25.mlp.gate_proj.lora_B.default.weight', 'backbone.model.base_model.model.layers.25.mlp.gate_proj.lora_B.test.weight', 'backbone.model.base_model.model.layers.25.mlp.up_proj.base_layer.weight', 'backbone.model.base_model.model.layers.25.mlp.up_proj.lora_A.default.weight', 'backbone.model.base_model.model.layers.25.mlp.up_proj.lora_A.test.weight', 'backbone.model.base_model.model.layers.25.mlp.up_proj.lora_B.default.weight', 'backbone.model.base_model.model.layers.25.mlp.up_proj.lora_B.test.weight', 'backbone.model.base_model.model.layers.25.mlp.down_proj.base_layer.weight', 'backbone.model.base_model.model.layers.25.mlp.down_proj.lora_A.default.weight', 'backbone.model.base_model.model.layers.25.mlp.down_proj.lora_A.test.weight', 'backbone.model.base_model.model.layers.25.mlp.down_proj.lora_B.default.weight', 'backbone.model.base_model.model.layers.25.mlp.down_proj.lora_B.test.weight', 'backbone.model.base_model.model.layers.25.input_layernorm.weight', 'backbone.model.base_model.model.layers.25.post_attention_layernorm.weight', 'backbone.model.base_model.model.layers.26.self_attn.q_proj.base_layer.weight', 'backbone.model.base_model.model.layers.26.self_attn.q_proj.lora_A.default.weight', 'backbone.model.base_model.model.layers.26.self_attn.q_proj.lora_A.test.weight', 'backbone.model.base_model.model.layers.26.self_attn.q_proj.lora_B.default.weight', 'backbone.model.base_model.model.layers.26.self_attn.q_proj.lora_B.test.weight', 'backbone.model.base_model.model.layers.26.self_attn.k_proj.base_layer.weight', 'backbone.model.base_model.model.layers.26.self_attn.k_proj.lora_A.default.weight', 'backbone.model.base_model.model.layers.26.self_attn.k_proj.lora_A.test.weight', 'backbone.model.base_model.model.layers.26.self_attn.k_proj.lora_B.default.weight', 'backbone.model.base_model.model.layers.26.self_attn.k_proj.lora_B.test.weight', 'backbone.model.base_model.model.layers.26.self_attn.v_proj.base_layer.weight', 'backbone.model.base_model.model.layers.26.self_attn.v_proj.lora_A.default.weight', 'backbone.model.base_model.model.layers.26.self_attn.v_proj.lora_A.test.weight', 'backbone.model.base_model.model.layers.26.self_attn.v_proj.lora_B.default.weight', 'backbone.model.base_model.model.layers.26.self_attn.v_proj.lora_B.test.weight', 'backbone.model.base_model.model.layers.26.self_attn.o_proj.base_layer.weight', 'backbone.model.base_model.model.layers.26.self_attn.o_proj.lora_A.default.weight', 'backbone.model.base_model.model.layers.26.self_attn.o_proj.lora_A.test.weight', 'backbone.model.base_model.model.layers.26.self_attn.o_proj.lora_B.default.weight', 'backbone.model.base_model.model.layers.26.self_attn.o_proj.lora_B.test.weight', 'backbone.model.base_model.model.layers.26.mlp.gate_proj.base_layer.weight', 'backbone.model.base_model.model.layers.26.mlp.gate_proj.lora_A.default.weight', 'backbone.model.base_model.model.layers.26.mlp.gate_proj.lora_A.test.weight', 'backbone.model.base_model.model.layers.26.mlp.gate_proj.lora_B.default.weight', 'backbone.model.base_model.model.layers.26.mlp.gate_proj.lora_B.test.weight', 'backbone.model.base_model.model.layers.26.mlp.up_proj.base_layer.weight', 'backbone.model.base_model.model.layers.26.mlp.up_proj.lora_A.default.weight', 'backbone.model.base_model.model.layers.26.mlp.up_proj.lora_A.test.weight', 'backbone.model.base_model.model.layers.26.mlp.up_proj.lora_B.default.weight', 'backbone.model.base_model.model.layers.26.mlp.up_proj.lora_B.test.weight', 'backbone.model.base_model.model.layers.26.mlp.down_proj.base_layer.weight', 'backbone.model.base_model.model.layers.26.mlp.down_proj.lora_A.default.weight', 'backbone.model.base_model.model.layers.26.mlp.down_proj.lora_A.test.weight', 'backbone.model.base_model.model.layers.26.mlp.down_proj.lora_B.default.weight', 'backbone.model.base_model.model.layers.26.mlp.down_proj.lora_B.test.weight', 'backbone.model.base_model.model.layers.26.input_layernorm.weight', 'backbone.model.base_model.model.layers.26.post_attention_layernorm.weight', 'backbone.model.base_model.model.layers.27.self_attn.q_proj.base_layer.weight', 'backbone.model.base_model.model.layers.27.self_attn.q_proj.lora_A.default.weight', 'backbone.model.base_model.model.layers.27.self_attn.q_proj.lora_A.test.weight', 'backbone.model.base_model.model.layers.27.self_attn.q_proj.lora_B.default.weight', 'backbone.model.base_model.model.layers.27.self_attn.q_proj.lora_B.test.weight', 'backbone.model.base_model.model.layers.27.self_attn.k_proj.base_layer.weight', 'backbone.model.base_model.model.layers.27.self_attn.k_proj.lora_A.default.weight', 'backbone.model.base_model.model.layers.27.self_attn.k_proj.lora_A.test.weight', 'backbone.model.base_model.model.layers.27.self_attn.k_proj.lora_B.default.weight', 'backbone.model.base_model.model.layers.27.self_attn.k_proj.lora_B.test.weight', 'backbone.model.base_model.model.layers.27.self_attn.v_proj.base_layer.weight', 'backbone.model.base_model.model.layers.27.self_attn.v_proj.lora_A.default.weight', 'backbone.model.base_model.model.layers.27.self_attn.v_proj.lora_A.test.weight', 'backbone.model.base_model.model.layers.27.self_attn.v_proj.lora_B.default.weight', 'backbone.model.base_model.model.layers.27.self_attn.v_proj.lora_B.test.weight', 'backbone.model.base_model.model.layers.27.self_attn.o_proj.base_layer.weight', 'backbone.model.base_model.model.layers.27.self_attn.o_proj.lora_A.default.weight', 'backbone.model.base_model.model.layers.27.self_attn.o_proj.lora_A.test.weight', 'backbone.model.base_model.model.layers.27.self_attn.o_proj.lora_B.default.weight', 'backbone.model.base_model.model.layers.27.self_attn.o_proj.lora_B.test.weight', 'backbone.model.base_model.model.layers.27.mlp.gate_proj.base_layer.weight', 'backbone.model.base_model.model.layers.27.mlp.gate_proj.lora_A.default.weight', 'backbone.model.base_model.model.layers.27.mlp.gate_proj.lora_A.test.weight', 'backbone.model.base_model.model.layers.27.mlp.gate_proj.lora_B.default.weight', 'backbone.model.base_model.model.layers.27.mlp.gate_proj.lora_B.test.weight', 'backbone.model.base_model.model.layers.27.mlp.up_proj.base_layer.weight', 'backbone.model.base_model.model.layers.27.mlp.up_proj.lora_A.default.weight', 'backbone.model.base_model.model.layers.27.mlp.up_proj.lora_A.test.weight', 'backbone.model.base_model.model.layers.27.mlp.up_proj.lora_B.default.weight', 'backbone.model.base_model.model.layers.27.mlp.up_proj.lora_B.test.weight', 'backbone.model.base_model.model.layers.27.mlp.down_proj.base_layer.weight', 'backbone.model.base_model.model.layers.27.mlp.down_proj.lora_A.default.weight', 'backbone.model.base_model.model.layers.27.mlp.down_proj.lora_A.test.weight', 'backbone.model.base_model.model.layers.27.mlp.down_proj.lora_B.default.weight', 'backbone.model.base_model.model.layers.27.mlp.down_proj.lora_B.test.weight', 'backbone.model.base_model.model.layers.27.input_layernorm.weight', 'backbone.model.base_model.model.layers.27.post_attention_layernorm.weight', 'backbone.model.base_model.model.layers.28.self_attn.q_proj.base_layer.weight', 'backbone.model.base_model.model.layers.28.self_attn.q_proj.lora_A.default.weight', 'backbone.model.base_model.model.layers.28.self_attn.q_proj.lora_A.test.weight', 'backbone.model.base_model.model.layers.28.self_attn.q_proj.lora_B.default.weight', 'backbone.model.base_model.model.layers.28.self_attn.q_proj.lora_B.test.weight', 'backbone.model.base_model.model.layers.28.self_attn.k_proj.base_layer.weight', 'backbone.model.base_model.model.layers.28.self_attn.k_proj.lora_A.default.weight', 'backbone.model.base_model.model.layers.28.self_attn.k_proj.lora_A.test.weight', 'backbone.model.base_model.model.layers.28.self_attn.k_proj.lora_B.default.weight', 'backbone.model.base_model.model.layers.28.self_attn.k_proj.lora_B.test.weight', 'backbone.model.base_model.model.layers.28.self_attn.v_proj.base_layer.weight', 'backbone.model.base_model.model.layers.28.self_attn.v_proj.lora_A.default.weight', 'backbone.model.base_model.model.layers.28.self_attn.v_proj.lora_A.test.weight', 'backbone.model.base_model.model.layers.28.self_attn.v_proj.lora_B.default.weight', 'backbone.model.base_model.model.layers.28.self_attn.v_proj.lora_B.test.weight', 'backbone.model.base_model.model.layers.28.self_attn.o_proj.base_layer.weight', 'backbone.model.base_model.model.layers.28.self_attn.o_proj.lora_A.default.weight', 'backbone.model.base_model.model.layers.28.self_attn.o_proj.lora_A.test.weight', 'backbone.model.base_model.model.layers.28.self_attn.o_proj.lora_B.default.weight', 'backbone.model.base_model.model.layers.28.self_attn.o_proj.lora_B.test.weight', 'backbone.model.base_model.model.layers.28.mlp.gate_proj.base_layer.weight', 'backbone.model.base_model.model.layers.28.mlp.gate_proj.lora_A.default.weight', 'backbone.model.base_model.model.layers.28.mlp.gate_proj.lora_A.test.weight', 'backbone.model.base_model.model.layers.28.mlp.gate_proj.lora_B.default.weight', 'backbone.model.base_model.model.layers.28.mlp.gate_proj.lora_B.test.weight', 'backbone.model.base_model.model.layers.28.mlp.up_proj.base_layer.weight', 'backbone.model.base_model.model.layers.28.mlp.up_proj.lora_A.default.weight', 'backbone.model.base_model.model.layers.28.mlp.up_proj.lora_A.test.weight', 'backbone.model.base_model.model.layers.28.mlp.up_proj.lora_B.default.weight', 'backbone.model.base_model.model.layers.28.mlp.up_proj.lora_B.test.weight', 'backbone.model.base_model.model.layers.28.mlp.down_proj.base_layer.weight', 'backbone.model.base_model.model.layers.28.mlp.down_proj.lora_A.default.weight', 'backbone.model.base_model.model.layers.28.mlp.down_proj.lora_A.test.weight', 'backbone.model.base_model.model.layers.28.mlp.down_proj.lora_B.default.weight', 'backbone.model.base_model.model.layers.28.mlp.down_proj.lora_B.test.weight', 'backbone.model.base_model.model.layers.28.input_layernorm.weight', 'backbone.model.base_model.model.layers.28.post_attention_layernorm.weight', 'backbone.model.base_model.model.layers.29.self_attn.q_proj.base_layer.weight', 'backbone.model.base_model.model.layers.29.self_attn.q_proj.lora_A.default.weight', 'backbone.model.base_model.model.layers.29.self_attn.q_proj.lora_A.test.weight', 'backbone.model.base_model.model.layers.29.self_attn.q_proj.lora_B.default.weight', 'backbone.model.base_model.model.layers.29.self_attn.q_proj.lora_B.test.weight', 'backbone.model.base_model.model.layers.29.self_attn.k_proj.base_layer.weight', 'backbone.model.base_model.model.layers.29.self_attn.k_proj.lora_A.default.weight', 'backbone.model.base_model.model.layers.29.self_attn.k_proj.lora_A.test.weight', 'backbone.model.base_model.model.layers.29.self_attn.k_proj.lora_B.default.weight', 'backbone.model.base_model.model.layers.29.self_attn.k_proj.lora_B.test.weight', 'backbone.model.base_model.model.layers.29.self_attn.v_proj.base_layer.weight', 'backbone.model.base_model.model.layers.29.self_attn.v_proj.lora_A.default.weight', 'backbone.model.base_model.model.layers.29.self_attn.v_proj.lora_A.test.weight', 'backbone.model.base_model.model.layers.29.self_attn.v_proj.lora_B.default.weight', 'backbone.model.base_model.model.layers.29.self_attn.v_proj.lora_B.test.weight', 'backbone.model.base_model.model.layers.29.self_attn.o_proj.base_layer.weight', 'backbone.model.base_model.model.layers.29.self_attn.o_proj.lora_A.default.weight', 'backbone.model.base_model.model.layers.29.self_attn.o_proj.lora_A.test.weight', 'backbone.model.base_model.model.layers.29.self_attn.o_proj.lora_B.default.weight', 'backbone.model.base_model.model.layers.29.self_attn.o_proj.lora_B.test.weight', 'backbone.model.base_model.model.layers.29.mlp.gate_proj.base_layer.weight', 'backbone.model.base_model.model.layers.29.mlp.gate_proj.lora_A.default.weight', 'backbone.model.base_model.model.layers.29.mlp.gate_proj.lora_A.test.weight', 'backbone.model.base_model.model.layers.29.mlp.gate_proj.lora_B.default.weight', 'backbone.model.base_model.model.layers.29.mlp.gate_proj.lora_B.test.weight', 'backbone.model.base_model.model.layers.29.mlp.up_proj.base_layer.weight', 'backbone.model.base_model.model.layers.29.mlp.up_proj.lora_A.default.weight', 'backbone.model.base_model.model.layers.29.mlp.up_proj.lora_A.test.weight', 'backbone.model.base_model.model.layers.29.mlp.up_proj.lora_B.default.weight', 'backbone.model.base_model.model.layers.29.mlp.up_proj.lora_B.test.weight', 'backbone.model.base_model.model.layers.29.mlp.down_proj.base_layer.weight', 'backbone.model.base_model.model.layers.29.mlp.down_proj.lora_A.default.weight', 'backbone.model.base_model.model.layers.29.mlp.down_proj.lora_A.test.weight', 'backbone.model.base_model.model.layers.29.mlp.down_proj.lora_B.default.weight', 'backbone.model.base_model.model.layers.29.mlp.down_proj.lora_B.test.weight', 'backbone.model.base_model.model.layers.29.input_layernorm.weight', 'backbone.model.base_model.model.layers.29.post_attention_layernorm.weight', 'backbone.model.base_model.model.layers.30.self_attn.q_proj.base_layer.weight', 'backbone.model.base_model.model.layers.30.self_attn.q_proj.lora_A.default.weight', 'backbone.model.base_model.model.layers.30.self_attn.q_proj.lora_A.test.weight', 'backbone.model.base_model.model.layers.30.self_attn.q_proj.lora_B.default.weight', 'backbone.model.base_model.model.layers.30.self_attn.q_proj.lora_B.test.weight', 'backbone.model.base_model.model.layers.30.self_attn.k_proj.base_layer.weight', 'backbone.model.base_model.model.layers.30.self_attn.k_proj.lora_A.default.weight', 'backbone.model.base_model.model.layers.30.self_attn.k_proj.lora_A.test.weight', 'backbone.model.base_model.model.layers.30.self_attn.k_proj.lora_B.default.weight', 'backbone.model.base_model.model.layers.30.self_attn.k_proj.lora_B.test.weight', 'backbone.model.base_model.model.layers.30.self_attn.v_proj.base_layer.weight', 'backbone.model.base_model.model.layers.30.self_attn.v_proj.lora_A.default.weight', 'backbone.model.base_model.model.layers.30.self_attn.v_proj.lora_A.test.weight', 'backbone.model.base_model.model.layers.30.self_attn.v_proj.lora_B.default.weight', 'backbone.model.base_model.model.layers.30.self_attn.v_proj.lora_B.test.weight', 'backbone.model.base_model.model.layers.30.self_attn.o_proj.base_layer.weight', 'backbone.model.base_model.model.layers.30.self_attn.o_proj.lora_A.default.weight', 'backbone.model.base_model.model.layers.30.self_attn.o_proj.lora_A.test.weight', 'backbone.model.base_model.model.layers.30.self_attn.o_proj.lora_B.default.weight', 'backbone.model.base_model.model.layers.30.self_attn.o_proj.lora_B.test.weight', 'backbone.model.base_model.model.layers.30.mlp.gate_proj.base_layer.weight', 'backbone.model.base_model.model.layers.30.mlp.gate_proj.lora_A.default.weight', 'backbone.model.base_model.model.layers.30.mlp.gate_proj.lora_A.test.weight', 'backbone.model.base_model.model.layers.30.mlp.gate_proj.lora_B.default.weight', 'backbone.model.base_model.model.layers.30.mlp.gate_proj.lora_B.test.weight', 'backbone.model.base_model.model.layers.30.mlp.up_proj.base_layer.weight', 'backbone.model.base_model.model.layers.30.mlp.up_proj.lora_A.default.weight', 'backbone.model.base_model.model.layers.30.mlp.up_proj.lora_A.test.weight', 'backbone.model.base_model.model.layers.30.mlp.up_proj.lora_B.default.weight', 'backbone.model.base_model.model.layers.30.mlp.up_proj.lora_B.test.weight', 'backbone.model.base_model.model.layers.30.mlp.down_proj.base_layer.weight', 'backbone.model.base_model.model.layers.30.mlp.down_proj.lora_A.default.weight', 'backbone.model.base_model.model.layers.30.mlp.down_proj.lora_A.test.weight', 'backbone.model.base_model.model.layers.30.mlp.down_proj.lora_B.default.weight', 'backbone.model.base_model.model.layers.30.mlp.down_proj.lora_B.test.weight', 'backbone.model.base_model.model.layers.30.input_layernorm.weight', 'backbone.model.base_model.model.layers.30.post_attention_layernorm.weight', 'backbone.model.base_model.model.layers.31.self_attn.q_proj.base_layer.weight', 'backbone.model.base_model.model.layers.31.self_attn.q_proj.lora_A.default.weight', 'backbone.model.base_model.model.layers.31.self_attn.q_proj.lora_A.test.weight', 'backbone.model.base_model.model.layers.31.self_attn.q_proj.lora_B.default.weight', 'backbone.model.base_model.model.layers.31.self_attn.q_proj.lora_B.test.weight', 'backbone.model.base_model.model.layers.31.self_attn.k_proj.base_layer.weight', 'backbone.model.base_model.model.layers.31.self_attn.k_proj.lora_A.default.weight', 'backbone.model.base_model.model.layers.31.self_attn.k_proj.lora_A.test.weight', 'backbone.model.base_model.model.layers.31.self_attn.k_proj.lora_B.default.weight', 'backbone.model.base_model.model.layers.31.self_attn.k_proj.lora_B.test.weight', 'backbone.model.base_model.model.layers.31.self_attn.v_proj.base_layer.weight', 'backbone.model.base_model.model.layers.31.self_attn.v_proj.lora_A.default.weight', 'backbone.model.base_model.model.layers.31.self_attn.v_proj.lora_A.test.weight', 'backbone.model.base_model.model.layers.31.self_attn.v_proj.lora_B.default.weight', 'backbone.model.base_model.model.layers.31.self_attn.v_proj.lora_B.test.weight', 'backbone.model.base_model.model.layers.31.self_attn.o_proj.base_layer.weight', 'backbone.model.base_model.model.layers.31.self_attn.o_proj.lora_A.default.weight', 'backbone.model.base_model.model.layers.31.self_attn.o_proj.lora_A.test.weight', 'backbone.model.base_model.model.layers.31.self_attn.o_proj.lora_B.default.weight', 'backbone.model.base_model.model.layers.31.self_attn.o_proj.lora_B.test.weight', 'backbone.model.base_model.model.layers.31.mlp.gate_proj.base_layer.weight', 'backbone.model.base_model.model.layers.31.mlp.gate_proj.lora_A.default.weight', 'backbone.model.base_model.model.layers.31.mlp.gate_proj.lora_A.test.weight', 'backbone.model.base_model.model.layers.31.mlp.gate_proj.lora_B.default.weight', 'backbone.model.base_model.model.layers.31.mlp.gate_proj.lora_B.test.weight', 'backbone.model.base_model.model.layers.31.mlp.up_proj.base_layer.weight', 'backbone.model.base_model.model.layers.31.mlp.up_proj.lora_A.default.weight', 'backbone.model.base_model.model.layers.31.mlp.up_proj.lora_A.test.weight', 'backbone.model.base_model.model.layers.31.mlp.up_proj.lora_B.default.weight', 'backbone.model.base_model.model.layers.31.mlp.up_proj.lora_B.test.weight', 'backbone.model.base_model.model.layers.31.mlp.down_proj.base_layer.weight', 'backbone.model.base_model.model.layers.31.mlp.down_proj.lora_A.default.weight', 'backbone.model.base_model.model.layers.31.mlp.down_proj.lora_A.test.weight', 'backbone.model.base_model.model.layers.31.mlp.down_proj.lora_B.default.weight', 'backbone.model.base_model.model.layers.31.mlp.down_proj.lora_B.test.weight', 'backbone.model.base_model.model.layers.31.input_layernorm.weight', 'backbone.model.base_model.model.layers.31.post_attention_layernorm.weight', 'backbone.model.base_model.model.norm.weight'], unexpected_keys=[])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(state_dict=state_dict, strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/anhnh/miniconda3/envs/commonenv/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca9769cfbced4a16b9447e60b0c4071b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4006d93881b849f89350132c850b91a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from llm2vec import LLM2Vec\n",
    "import torch\n",
    "\n",
    "backbone = LLM2Vec.from_pretrained(\n",
    "            \"McGill-NLP/LLM2Vec-Meta-Llama-3-8B-Instruct-mntp\",\n",
    "            peft_model_name_or_path=\"McGill-NLP/LLM2Vec-Meta-Llama-3-8B-Instruct-mntp-supervised\",\n",
    "            device_map=\"cuda:0\",\n",
    "            torch_dtype=torch.bfloat16,\n",
    "            merge_peft=True,\n",
    "            pooling_mode=\"mean\",\n",
    "            max_length=120,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "backbone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaBiModel(\n",
       "  (embed_tokens): Embedding(128256, 4096)\n",
       "  (layers): ModuleList(\n",
       "    (0-31): 32 x ModifiedLlamaDecoderLayer(\n",
       "      (self_attn): ModifiedLlamaSdpaAttention(\n",
       "        (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "        (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "        (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (rotary_emb): LlamaRotaryEmbedding()\n",
       "      )\n",
       "      (mlp): LlamaMLP(\n",
       "        (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "        (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "        (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "        (act_fn): SiLU()\n",
       "      )\n",
       "      (input_layernorm): LlamaRMSNorm()\n",
       "      (post_attention_layernorm): LlamaRMSNorm()\n",
       "    )\n",
       "  )\n",
       "  (norm): LlamaRMSNorm()\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "backbone.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "\n",
    "backbone.model = PeftModel.from_pretrained(backbone.model, \"./outputs/early_stop/test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModel(\n",
       "  (base_model): LoraModel(\n",
       "    (model): LlamaBiModel(\n",
       "      (embed_tokens): Embedding(128256, 4096)\n",
       "      (layers): ModuleList(\n",
       "        (0-31): 32 x ModifiedLlamaDecoderLayer(\n",
       "          (self_attn): ModifiedLlamaSdpaAttention(\n",
       "            (q_proj): lora.Linear(\n",
       "              (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "              (lora_dropout): ModuleDict(\n",
       "                (default): Dropout(p=0.05, inplace=False)\n",
       "              )\n",
       "              (lora_A): ModuleDict(\n",
       "                (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "              )\n",
       "              (lora_B): ModuleDict(\n",
       "                (default): Linear(in_features=8, out_features=4096, bias=False)\n",
       "              )\n",
       "              (lora_embedding_A): ParameterDict()\n",
       "              (lora_embedding_B): ParameterDict()\n",
       "            )\n",
       "            (k_proj): lora.Linear(\n",
       "              (base_layer): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "              (lora_dropout): ModuleDict(\n",
       "                (default): Dropout(p=0.05, inplace=False)\n",
       "              )\n",
       "              (lora_A): ModuleDict(\n",
       "                (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "              )\n",
       "              (lora_B): ModuleDict(\n",
       "                (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "              )\n",
       "              (lora_embedding_A): ParameterDict()\n",
       "              (lora_embedding_B): ParameterDict()\n",
       "            )\n",
       "            (v_proj): lora.Linear(\n",
       "              (base_layer): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "              (lora_dropout): ModuleDict(\n",
       "                (default): Dropout(p=0.05, inplace=False)\n",
       "              )\n",
       "              (lora_A): ModuleDict(\n",
       "                (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "              )\n",
       "              (lora_B): ModuleDict(\n",
       "                (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "              )\n",
       "              (lora_embedding_A): ParameterDict()\n",
       "              (lora_embedding_B): ParameterDict()\n",
       "            )\n",
       "            (o_proj): lora.Linear(\n",
       "              (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "              (lora_dropout): ModuleDict(\n",
       "                (default): Dropout(p=0.05, inplace=False)\n",
       "              )\n",
       "              (lora_A): ModuleDict(\n",
       "                (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "              )\n",
       "              (lora_B): ModuleDict(\n",
       "                (default): Linear(in_features=8, out_features=4096, bias=False)\n",
       "              )\n",
       "              (lora_embedding_A): ParameterDict()\n",
       "              (lora_embedding_B): ParameterDict()\n",
       "            )\n",
       "            (rotary_emb): LlamaRotaryEmbedding()\n",
       "          )\n",
       "          (mlp): LlamaMLP(\n",
       "            (gate_proj): lora.Linear(\n",
       "              (base_layer): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "              (lora_dropout): ModuleDict(\n",
       "                (default): Dropout(p=0.05, inplace=False)\n",
       "              )\n",
       "              (lora_A): ModuleDict(\n",
       "                (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "              )\n",
       "              (lora_B): ModuleDict(\n",
       "                (default): Linear(in_features=8, out_features=14336, bias=False)\n",
       "              )\n",
       "              (lora_embedding_A): ParameterDict()\n",
       "              (lora_embedding_B): ParameterDict()\n",
       "            )\n",
       "            (up_proj): lora.Linear(\n",
       "              (base_layer): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "              (lora_dropout): ModuleDict(\n",
       "                (default): Dropout(p=0.05, inplace=False)\n",
       "              )\n",
       "              (lora_A): ModuleDict(\n",
       "                (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "              )\n",
       "              (lora_B): ModuleDict(\n",
       "                (default): Linear(in_features=8, out_features=14336, bias=False)\n",
       "              )\n",
       "              (lora_embedding_A): ParameterDict()\n",
       "              (lora_embedding_B): ParameterDict()\n",
       "            )\n",
       "            (down_proj): lora.Linear(\n",
       "              (base_layer): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "              (lora_dropout): ModuleDict(\n",
       "                (default): Dropout(p=0.05, inplace=False)\n",
       "              )\n",
       "              (lora_A): ModuleDict(\n",
       "                (default): Linear(in_features=14336, out_features=8, bias=False)\n",
       "              )\n",
       "              (lora_B): ModuleDict(\n",
       "                (default): Linear(in_features=8, out_features=4096, bias=False)\n",
       "              )\n",
       "              (lora_embedding_A): ParameterDict()\n",
       "              (lora_embedding_B): ParameterDict()\n",
       "            )\n",
       "            (act_fn): SiLU()\n",
       "          )\n",
       "          (input_layernorm): LlamaRMSNorm()\n",
       "          (post_attention_layernorm): LlamaRMSNorm()\n",
       "        )\n",
       "      )\n",
       "      (norm): LlamaRMSNorm()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "backbone.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.backbone.model.load_adapter(\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_IncompatibleKeys(missing_keys=['base_model.model.embed_tokens.weight', 'base_model.model.layers.0.self_attn.q_proj.base_layer.weight', 'base_model.model.layers.0.self_attn.q_proj.lora_A.default.weight', 'base_model.model.layers.0.self_attn.q_proj.lora_B.default.weight', 'base_model.model.layers.0.self_attn.k_proj.base_layer.weight', 'base_model.model.layers.0.self_attn.k_proj.lora_A.default.weight', 'base_model.model.layers.0.self_attn.k_proj.lora_B.default.weight', 'base_model.model.layers.0.self_attn.v_proj.base_layer.weight', 'base_model.model.layers.0.self_attn.v_proj.lora_A.default.weight', 'base_model.model.layers.0.self_attn.v_proj.lora_B.default.weight', 'base_model.model.layers.0.self_attn.o_proj.base_layer.weight', 'base_model.model.layers.0.self_attn.o_proj.lora_A.default.weight', 'base_model.model.layers.0.self_attn.o_proj.lora_B.default.weight', 'base_model.model.layers.0.mlp.gate_proj.base_layer.weight', 'base_model.model.layers.0.mlp.gate_proj.lora_A.default.weight', 'base_model.model.layers.0.mlp.gate_proj.lora_B.default.weight', 'base_model.model.layers.0.mlp.up_proj.base_layer.weight', 'base_model.model.layers.0.mlp.up_proj.lora_A.default.weight', 'base_model.model.layers.0.mlp.up_proj.lora_B.default.weight', 'base_model.model.layers.0.mlp.down_proj.base_layer.weight', 'base_model.model.layers.0.mlp.down_proj.lora_A.default.weight', 'base_model.model.layers.0.mlp.down_proj.lora_B.default.weight', 'base_model.model.layers.0.input_layernorm.weight', 'base_model.model.layers.0.post_attention_layernorm.weight', 'base_model.model.layers.1.self_attn.q_proj.base_layer.weight', 'base_model.model.layers.1.self_attn.q_proj.lora_A.default.weight', 'base_model.model.layers.1.self_attn.q_proj.lora_B.default.weight', 'base_model.model.layers.1.self_attn.k_proj.base_layer.weight', 'base_model.model.layers.1.self_attn.k_proj.lora_A.default.weight', 'base_model.model.layers.1.self_attn.k_proj.lora_B.default.weight', 'base_model.model.layers.1.self_attn.v_proj.base_layer.weight', 'base_model.model.layers.1.self_attn.v_proj.lora_A.default.weight', 'base_model.model.layers.1.self_attn.v_proj.lora_B.default.weight', 'base_model.model.layers.1.self_attn.o_proj.base_layer.weight', 'base_model.model.layers.1.self_attn.o_proj.lora_A.default.weight', 'base_model.model.layers.1.self_attn.o_proj.lora_B.default.weight', 'base_model.model.layers.1.mlp.gate_proj.base_layer.weight', 'base_model.model.layers.1.mlp.gate_proj.lora_A.default.weight', 'base_model.model.layers.1.mlp.gate_proj.lora_B.default.weight', 'base_model.model.layers.1.mlp.up_proj.base_layer.weight', 'base_model.model.layers.1.mlp.up_proj.lora_A.default.weight', 'base_model.model.layers.1.mlp.up_proj.lora_B.default.weight', 'base_model.model.layers.1.mlp.down_proj.base_layer.weight', 'base_model.model.layers.1.mlp.down_proj.lora_A.default.weight', 'base_model.model.layers.1.mlp.down_proj.lora_B.default.weight', 'base_model.model.layers.1.input_layernorm.weight', 'base_model.model.layers.1.post_attention_layernorm.weight', 'base_model.model.layers.2.self_attn.q_proj.base_layer.weight', 'base_model.model.layers.2.self_attn.q_proj.lora_A.default.weight', 'base_model.model.layers.2.self_attn.q_proj.lora_B.default.weight', 'base_model.model.layers.2.self_attn.k_proj.base_layer.weight', 'base_model.model.layers.2.self_attn.k_proj.lora_A.default.weight', 'base_model.model.layers.2.self_attn.k_proj.lora_B.default.weight', 'base_model.model.layers.2.self_attn.v_proj.base_layer.weight', 'base_model.model.layers.2.self_attn.v_proj.lora_A.default.weight', 'base_model.model.layers.2.self_attn.v_proj.lora_B.default.weight', 'base_model.model.layers.2.self_attn.o_proj.base_layer.weight', 'base_model.model.layers.2.self_attn.o_proj.lora_A.default.weight', 'base_model.model.layers.2.self_attn.o_proj.lora_B.default.weight', 'base_model.model.layers.2.mlp.gate_proj.base_layer.weight', 'base_model.model.layers.2.mlp.gate_proj.lora_A.default.weight', 'base_model.model.layers.2.mlp.gate_proj.lora_B.default.weight', 'base_model.model.layers.2.mlp.up_proj.base_layer.weight', 'base_model.model.layers.2.mlp.up_proj.lora_A.default.weight', 'base_model.model.layers.2.mlp.up_proj.lora_B.default.weight', 'base_model.model.layers.2.mlp.down_proj.base_layer.weight', 'base_model.model.layers.2.mlp.down_proj.lora_A.default.weight', 'base_model.model.layers.2.mlp.down_proj.lora_B.default.weight', 'base_model.model.layers.2.input_layernorm.weight', 'base_model.model.layers.2.post_attention_layernorm.weight', 'base_model.model.layers.3.self_attn.q_proj.base_layer.weight', 'base_model.model.layers.3.self_attn.q_proj.lora_A.default.weight', 'base_model.model.layers.3.self_attn.q_proj.lora_B.default.weight', 'base_model.model.layers.3.self_attn.k_proj.base_layer.weight', 'base_model.model.layers.3.self_attn.k_proj.lora_A.default.weight', 'base_model.model.layers.3.self_attn.k_proj.lora_B.default.weight', 'base_model.model.layers.3.self_attn.v_proj.base_layer.weight', 'base_model.model.layers.3.self_attn.v_proj.lora_A.default.weight', 'base_model.model.layers.3.self_attn.v_proj.lora_B.default.weight', 'base_model.model.layers.3.self_attn.o_proj.base_layer.weight', 'base_model.model.layers.3.self_attn.o_proj.lora_A.default.weight', 'base_model.model.layers.3.self_attn.o_proj.lora_B.default.weight', 'base_model.model.layers.3.mlp.gate_proj.base_layer.weight', 'base_model.model.layers.3.mlp.gate_proj.lora_A.default.weight', 'base_model.model.layers.3.mlp.gate_proj.lora_B.default.weight', 'base_model.model.layers.3.mlp.up_proj.base_layer.weight', 'base_model.model.layers.3.mlp.up_proj.lora_A.default.weight', 'base_model.model.layers.3.mlp.up_proj.lora_B.default.weight', 'base_model.model.layers.3.mlp.down_proj.base_layer.weight', 'base_model.model.layers.3.mlp.down_proj.lora_A.default.weight', 'base_model.model.layers.3.mlp.down_proj.lora_B.default.weight', 'base_model.model.layers.3.input_layernorm.weight', 'base_model.model.layers.3.post_attention_layernorm.weight', 'base_model.model.layers.4.self_attn.q_proj.base_layer.weight', 'base_model.model.layers.4.self_attn.q_proj.lora_A.default.weight', 'base_model.model.layers.4.self_attn.q_proj.lora_B.default.weight', 'base_model.model.layers.4.self_attn.k_proj.base_layer.weight', 'base_model.model.layers.4.self_attn.k_proj.lora_A.default.weight', 'base_model.model.layers.4.self_attn.k_proj.lora_B.default.weight', 'base_model.model.layers.4.self_attn.v_proj.base_layer.weight', 'base_model.model.layers.4.self_attn.v_proj.lora_A.default.weight', 'base_model.model.layers.4.self_attn.v_proj.lora_B.default.weight', 'base_model.model.layers.4.self_attn.o_proj.base_layer.weight', 'base_model.model.layers.4.self_attn.o_proj.lora_A.default.weight', 'base_model.model.layers.4.self_attn.o_proj.lora_B.default.weight', 'base_model.model.layers.4.mlp.gate_proj.base_layer.weight', 'base_model.model.layers.4.mlp.gate_proj.lora_A.default.weight', 'base_model.model.layers.4.mlp.gate_proj.lora_B.default.weight', 'base_model.model.layers.4.mlp.up_proj.base_layer.weight', 'base_model.model.layers.4.mlp.up_proj.lora_A.default.weight', 'base_model.model.layers.4.mlp.up_proj.lora_B.default.weight', 'base_model.model.layers.4.mlp.down_proj.base_layer.weight', 'base_model.model.layers.4.mlp.down_proj.lora_A.default.weight', 'base_model.model.layers.4.mlp.down_proj.lora_B.default.weight', 'base_model.model.layers.4.input_layernorm.weight', 'base_model.model.layers.4.post_attention_layernorm.weight', 'base_model.model.layers.5.self_attn.q_proj.base_layer.weight', 'base_model.model.layers.5.self_attn.q_proj.lora_A.default.weight', 'base_model.model.layers.5.self_attn.q_proj.lora_B.default.weight', 'base_model.model.layers.5.self_attn.k_proj.base_layer.weight', 'base_model.model.layers.5.self_attn.k_proj.lora_A.default.weight', 'base_model.model.layers.5.self_attn.k_proj.lora_B.default.weight', 'base_model.model.layers.5.self_attn.v_proj.base_layer.weight', 'base_model.model.layers.5.self_attn.v_proj.lora_A.default.weight', 'base_model.model.layers.5.self_attn.v_proj.lora_B.default.weight', 'base_model.model.layers.5.self_attn.o_proj.base_layer.weight', 'base_model.model.layers.5.self_attn.o_proj.lora_A.default.weight', 'base_model.model.layers.5.self_attn.o_proj.lora_B.default.weight', 'base_model.model.layers.5.mlp.gate_proj.base_layer.weight', 'base_model.model.layers.5.mlp.gate_proj.lora_A.default.weight', 'base_model.model.layers.5.mlp.gate_proj.lora_B.default.weight', 'base_model.model.layers.5.mlp.up_proj.base_layer.weight', 'base_model.model.layers.5.mlp.up_proj.lora_A.default.weight', 'base_model.model.layers.5.mlp.up_proj.lora_B.default.weight', 'base_model.model.layers.5.mlp.down_proj.base_layer.weight', 'base_model.model.layers.5.mlp.down_proj.lora_A.default.weight', 'base_model.model.layers.5.mlp.down_proj.lora_B.default.weight', 'base_model.model.layers.5.input_layernorm.weight', 'base_model.model.layers.5.post_attention_layernorm.weight', 'base_model.model.layers.6.self_attn.q_proj.base_layer.weight', 'base_model.model.layers.6.self_attn.q_proj.lora_A.default.weight', 'base_model.model.layers.6.self_attn.q_proj.lora_B.default.weight', 'base_model.model.layers.6.self_attn.k_proj.base_layer.weight', 'base_model.model.layers.6.self_attn.k_proj.lora_A.default.weight', 'base_model.model.layers.6.self_attn.k_proj.lora_B.default.weight', 'base_model.model.layers.6.self_attn.v_proj.base_layer.weight', 'base_model.model.layers.6.self_attn.v_proj.lora_A.default.weight', 'base_model.model.layers.6.self_attn.v_proj.lora_B.default.weight', 'base_model.model.layers.6.self_attn.o_proj.base_layer.weight', 'base_model.model.layers.6.self_attn.o_proj.lora_A.default.weight', 'base_model.model.layers.6.self_attn.o_proj.lora_B.default.weight', 'base_model.model.layers.6.mlp.gate_proj.base_layer.weight', 'base_model.model.layers.6.mlp.gate_proj.lora_A.default.weight', 'base_model.model.layers.6.mlp.gate_proj.lora_B.default.weight', 'base_model.model.layers.6.mlp.up_proj.base_layer.weight', 'base_model.model.layers.6.mlp.up_proj.lora_A.default.weight', 'base_model.model.layers.6.mlp.up_proj.lora_B.default.weight', 'base_model.model.layers.6.mlp.down_proj.base_layer.weight', 'base_model.model.layers.6.mlp.down_proj.lora_A.default.weight', 'base_model.model.layers.6.mlp.down_proj.lora_B.default.weight', 'base_model.model.layers.6.input_layernorm.weight', 'base_model.model.layers.6.post_attention_layernorm.weight', 'base_model.model.layers.7.self_attn.q_proj.base_layer.weight', 'base_model.model.layers.7.self_attn.q_proj.lora_A.default.weight', 'base_model.model.layers.7.self_attn.q_proj.lora_B.default.weight', 'base_model.model.layers.7.self_attn.k_proj.base_layer.weight', 'base_model.model.layers.7.self_attn.k_proj.lora_A.default.weight', 'base_model.model.layers.7.self_attn.k_proj.lora_B.default.weight', 'base_model.model.layers.7.self_attn.v_proj.base_layer.weight', 'base_model.model.layers.7.self_attn.v_proj.lora_A.default.weight', 'base_model.model.layers.7.self_attn.v_proj.lora_B.default.weight', 'base_model.model.layers.7.self_attn.o_proj.base_layer.weight', 'base_model.model.layers.7.self_attn.o_proj.lora_A.default.weight', 'base_model.model.layers.7.self_attn.o_proj.lora_B.default.weight', 'base_model.model.layers.7.mlp.gate_proj.base_layer.weight', 'base_model.model.layers.7.mlp.gate_proj.lora_A.default.weight', 'base_model.model.layers.7.mlp.gate_proj.lora_B.default.weight', 'base_model.model.layers.7.mlp.up_proj.base_layer.weight', 'base_model.model.layers.7.mlp.up_proj.lora_A.default.weight', 'base_model.model.layers.7.mlp.up_proj.lora_B.default.weight', 'base_model.model.layers.7.mlp.down_proj.base_layer.weight', 'base_model.model.layers.7.mlp.down_proj.lora_A.default.weight', 'base_model.model.layers.7.mlp.down_proj.lora_B.default.weight', 'base_model.model.layers.7.input_layernorm.weight', 'base_model.model.layers.7.post_attention_layernorm.weight', 'base_model.model.layers.8.self_attn.q_proj.base_layer.weight', 'base_model.model.layers.8.self_attn.q_proj.lora_A.default.weight', 'base_model.model.layers.8.self_attn.q_proj.lora_B.default.weight', 'base_model.model.layers.8.self_attn.k_proj.base_layer.weight', 'base_model.model.layers.8.self_attn.k_proj.lora_A.default.weight', 'base_model.model.layers.8.self_attn.k_proj.lora_B.default.weight', 'base_model.model.layers.8.self_attn.v_proj.base_layer.weight', 'base_model.model.layers.8.self_attn.v_proj.lora_A.default.weight', 'base_model.model.layers.8.self_attn.v_proj.lora_B.default.weight', 'base_model.model.layers.8.self_attn.o_proj.base_layer.weight', 'base_model.model.layers.8.self_attn.o_proj.lora_A.default.weight', 'base_model.model.layers.8.self_attn.o_proj.lora_B.default.weight', 'base_model.model.layers.8.mlp.gate_proj.base_layer.weight', 'base_model.model.layers.8.mlp.gate_proj.lora_A.default.weight', 'base_model.model.layers.8.mlp.gate_proj.lora_B.default.weight', 'base_model.model.layers.8.mlp.up_proj.base_layer.weight', 'base_model.model.layers.8.mlp.up_proj.lora_A.default.weight', 'base_model.model.layers.8.mlp.up_proj.lora_B.default.weight', 'base_model.model.layers.8.mlp.down_proj.base_layer.weight', 'base_model.model.layers.8.mlp.down_proj.lora_A.default.weight', 'base_model.model.layers.8.mlp.down_proj.lora_B.default.weight', 'base_model.model.layers.8.input_layernorm.weight', 'base_model.model.layers.8.post_attention_layernorm.weight', 'base_model.model.layers.9.self_attn.q_proj.base_layer.weight', 'base_model.model.layers.9.self_attn.q_proj.lora_A.default.weight', 'base_model.model.layers.9.self_attn.q_proj.lora_B.default.weight', 'base_model.model.layers.9.self_attn.k_proj.base_layer.weight', 'base_model.model.layers.9.self_attn.k_proj.lora_A.default.weight', 'base_model.model.layers.9.self_attn.k_proj.lora_B.default.weight', 'base_model.model.layers.9.self_attn.v_proj.base_layer.weight', 'base_model.model.layers.9.self_attn.v_proj.lora_A.default.weight', 'base_model.model.layers.9.self_attn.v_proj.lora_B.default.weight', 'base_model.model.layers.9.self_attn.o_proj.base_layer.weight', 'base_model.model.layers.9.self_attn.o_proj.lora_A.default.weight', 'base_model.model.layers.9.self_attn.o_proj.lora_B.default.weight', 'base_model.model.layers.9.mlp.gate_proj.base_layer.weight', 'base_model.model.layers.9.mlp.gate_proj.lora_A.default.weight', 'base_model.model.layers.9.mlp.gate_proj.lora_B.default.weight', 'base_model.model.layers.9.mlp.up_proj.base_layer.weight', 'base_model.model.layers.9.mlp.up_proj.lora_A.default.weight', 'base_model.model.layers.9.mlp.up_proj.lora_B.default.weight', 'base_model.model.layers.9.mlp.down_proj.base_layer.weight', 'base_model.model.layers.9.mlp.down_proj.lora_A.default.weight', 'base_model.model.layers.9.mlp.down_proj.lora_B.default.weight', 'base_model.model.layers.9.input_layernorm.weight', 'base_model.model.layers.9.post_attention_layernorm.weight', 'base_model.model.layers.10.self_attn.q_proj.base_layer.weight', 'base_model.model.layers.10.self_attn.q_proj.lora_A.default.weight', 'base_model.model.layers.10.self_attn.q_proj.lora_B.default.weight', 'base_model.model.layers.10.self_attn.k_proj.base_layer.weight', 'base_model.model.layers.10.self_attn.k_proj.lora_A.default.weight', 'base_model.model.layers.10.self_attn.k_proj.lora_B.default.weight', 'base_model.model.layers.10.self_attn.v_proj.base_layer.weight', 'base_model.model.layers.10.self_attn.v_proj.lora_A.default.weight', 'base_model.model.layers.10.self_attn.v_proj.lora_B.default.weight', 'base_model.model.layers.10.self_attn.o_proj.base_layer.weight', 'base_model.model.layers.10.self_attn.o_proj.lora_A.default.weight', 'base_model.model.layers.10.self_attn.o_proj.lora_B.default.weight', 'base_model.model.layers.10.mlp.gate_proj.base_layer.weight', 'base_model.model.layers.10.mlp.gate_proj.lora_A.default.weight', 'base_model.model.layers.10.mlp.gate_proj.lora_B.default.weight', 'base_model.model.layers.10.mlp.up_proj.base_layer.weight', 'base_model.model.layers.10.mlp.up_proj.lora_A.default.weight', 'base_model.model.layers.10.mlp.up_proj.lora_B.default.weight', 'base_model.model.layers.10.mlp.down_proj.base_layer.weight', 'base_model.model.layers.10.mlp.down_proj.lora_A.default.weight', 'base_model.model.layers.10.mlp.down_proj.lora_B.default.weight', 'base_model.model.layers.10.input_layernorm.weight', 'base_model.model.layers.10.post_attention_layernorm.weight', 'base_model.model.layers.11.self_attn.q_proj.base_layer.weight', 'base_model.model.layers.11.self_attn.q_proj.lora_A.default.weight', 'base_model.model.layers.11.self_attn.q_proj.lora_B.default.weight', 'base_model.model.layers.11.self_attn.k_proj.base_layer.weight', 'base_model.model.layers.11.self_attn.k_proj.lora_A.default.weight', 'base_model.model.layers.11.self_attn.k_proj.lora_B.default.weight', 'base_model.model.layers.11.self_attn.v_proj.base_layer.weight', 'base_model.model.layers.11.self_attn.v_proj.lora_A.default.weight', 'base_model.model.layers.11.self_attn.v_proj.lora_B.default.weight', 'base_model.model.layers.11.self_attn.o_proj.base_layer.weight', 'base_model.model.layers.11.self_attn.o_proj.lora_A.default.weight', 'base_model.model.layers.11.self_attn.o_proj.lora_B.default.weight', 'base_model.model.layers.11.mlp.gate_proj.base_layer.weight', 'base_model.model.layers.11.mlp.gate_proj.lora_A.default.weight', 'base_model.model.layers.11.mlp.gate_proj.lora_B.default.weight', 'base_model.model.layers.11.mlp.up_proj.base_layer.weight', 'base_model.model.layers.11.mlp.up_proj.lora_A.default.weight', 'base_model.model.layers.11.mlp.up_proj.lora_B.default.weight', 'base_model.model.layers.11.mlp.down_proj.base_layer.weight', 'base_model.model.layers.11.mlp.down_proj.lora_A.default.weight', 'base_model.model.layers.11.mlp.down_proj.lora_B.default.weight', 'base_model.model.layers.11.input_layernorm.weight', 'base_model.model.layers.11.post_attention_layernorm.weight', 'base_model.model.layers.12.self_attn.q_proj.base_layer.weight', 'base_model.model.layers.12.self_attn.q_proj.lora_A.default.weight', 'base_model.model.layers.12.self_attn.q_proj.lora_B.default.weight', 'base_model.model.layers.12.self_attn.k_proj.base_layer.weight', 'base_model.model.layers.12.self_attn.k_proj.lora_A.default.weight', 'base_model.model.layers.12.self_attn.k_proj.lora_B.default.weight', 'base_model.model.layers.12.self_attn.v_proj.base_layer.weight', 'base_model.model.layers.12.self_attn.v_proj.lora_A.default.weight', 'base_model.model.layers.12.self_attn.v_proj.lora_B.default.weight', 'base_model.model.layers.12.self_attn.o_proj.base_layer.weight', 'base_model.model.layers.12.self_attn.o_proj.lora_A.default.weight', 'base_model.model.layers.12.self_attn.o_proj.lora_B.default.weight', 'base_model.model.layers.12.mlp.gate_proj.base_layer.weight', 'base_model.model.layers.12.mlp.gate_proj.lora_A.default.weight', 'base_model.model.layers.12.mlp.gate_proj.lora_B.default.weight', 'base_model.model.layers.12.mlp.up_proj.base_layer.weight', 'base_model.model.layers.12.mlp.up_proj.lora_A.default.weight', 'base_model.model.layers.12.mlp.up_proj.lora_B.default.weight', 'base_model.model.layers.12.mlp.down_proj.base_layer.weight', 'base_model.model.layers.12.mlp.down_proj.lora_A.default.weight', 'base_model.model.layers.12.mlp.down_proj.lora_B.default.weight', 'base_model.model.layers.12.input_layernorm.weight', 'base_model.model.layers.12.post_attention_layernorm.weight', 'base_model.model.layers.13.self_attn.q_proj.base_layer.weight', 'base_model.model.layers.13.self_attn.q_proj.lora_A.default.weight', 'base_model.model.layers.13.self_attn.q_proj.lora_B.default.weight', 'base_model.model.layers.13.self_attn.k_proj.base_layer.weight', 'base_model.model.layers.13.self_attn.k_proj.lora_A.default.weight', 'base_model.model.layers.13.self_attn.k_proj.lora_B.default.weight', 'base_model.model.layers.13.self_attn.v_proj.base_layer.weight', 'base_model.model.layers.13.self_attn.v_proj.lora_A.default.weight', 'base_model.model.layers.13.self_attn.v_proj.lora_B.default.weight', 'base_model.model.layers.13.self_attn.o_proj.base_layer.weight', 'base_model.model.layers.13.self_attn.o_proj.lora_A.default.weight', 'base_model.model.layers.13.self_attn.o_proj.lora_B.default.weight', 'base_model.model.layers.13.mlp.gate_proj.base_layer.weight', 'base_model.model.layers.13.mlp.gate_proj.lora_A.default.weight', 'base_model.model.layers.13.mlp.gate_proj.lora_B.default.weight', 'base_model.model.layers.13.mlp.up_proj.base_layer.weight', 'base_model.model.layers.13.mlp.up_proj.lora_A.default.weight', 'base_model.model.layers.13.mlp.up_proj.lora_B.default.weight', 'base_model.model.layers.13.mlp.down_proj.base_layer.weight', 'base_model.model.layers.13.mlp.down_proj.lora_A.default.weight', 'base_model.model.layers.13.mlp.down_proj.lora_B.default.weight', 'base_model.model.layers.13.input_layernorm.weight', 'base_model.model.layers.13.post_attention_layernorm.weight', 'base_model.model.layers.14.self_attn.q_proj.base_layer.weight', 'base_model.model.layers.14.self_attn.q_proj.lora_A.default.weight', 'base_model.model.layers.14.self_attn.q_proj.lora_B.default.weight', 'base_model.model.layers.14.self_attn.k_proj.base_layer.weight', 'base_model.model.layers.14.self_attn.k_proj.lora_A.default.weight', 'base_model.model.layers.14.self_attn.k_proj.lora_B.default.weight', 'base_model.model.layers.14.self_attn.v_proj.base_layer.weight', 'base_model.model.layers.14.self_attn.v_proj.lora_A.default.weight', 'base_model.model.layers.14.self_attn.v_proj.lora_B.default.weight', 'base_model.model.layers.14.self_attn.o_proj.base_layer.weight', 'base_model.model.layers.14.self_attn.o_proj.lora_A.default.weight', 'base_model.model.layers.14.self_attn.o_proj.lora_B.default.weight', 'base_model.model.layers.14.mlp.gate_proj.base_layer.weight', 'base_model.model.layers.14.mlp.gate_proj.lora_A.default.weight', 'base_model.model.layers.14.mlp.gate_proj.lora_B.default.weight', 'base_model.model.layers.14.mlp.up_proj.base_layer.weight', 'base_model.model.layers.14.mlp.up_proj.lora_A.default.weight', 'base_model.model.layers.14.mlp.up_proj.lora_B.default.weight', 'base_model.model.layers.14.mlp.down_proj.base_layer.weight', 'base_model.model.layers.14.mlp.down_proj.lora_A.default.weight', 'base_model.model.layers.14.mlp.down_proj.lora_B.default.weight', 'base_model.model.layers.14.input_layernorm.weight', 'base_model.model.layers.14.post_attention_layernorm.weight', 'base_model.model.layers.15.self_attn.q_proj.base_layer.weight', 'base_model.model.layers.15.self_attn.q_proj.lora_A.default.weight', 'base_model.model.layers.15.self_attn.q_proj.lora_B.default.weight', 'base_model.model.layers.15.self_attn.k_proj.base_layer.weight', 'base_model.model.layers.15.self_attn.k_proj.lora_A.default.weight', 'base_model.model.layers.15.self_attn.k_proj.lora_B.default.weight', 'base_model.model.layers.15.self_attn.v_proj.base_layer.weight', 'base_model.model.layers.15.self_attn.v_proj.lora_A.default.weight', 'base_model.model.layers.15.self_attn.v_proj.lora_B.default.weight', 'base_model.model.layers.15.self_attn.o_proj.base_layer.weight', 'base_model.model.layers.15.self_attn.o_proj.lora_A.default.weight', 'base_model.model.layers.15.self_attn.o_proj.lora_B.default.weight', 'base_model.model.layers.15.mlp.gate_proj.base_layer.weight', 'base_model.model.layers.15.mlp.gate_proj.lora_A.default.weight', 'base_model.model.layers.15.mlp.gate_proj.lora_B.default.weight', 'base_model.model.layers.15.mlp.up_proj.base_layer.weight', 'base_model.model.layers.15.mlp.up_proj.lora_A.default.weight', 'base_model.model.layers.15.mlp.up_proj.lora_B.default.weight', 'base_model.model.layers.15.mlp.down_proj.base_layer.weight', 'base_model.model.layers.15.mlp.down_proj.lora_A.default.weight', 'base_model.model.layers.15.mlp.down_proj.lora_B.default.weight', 'base_model.model.layers.15.input_layernorm.weight', 'base_model.model.layers.15.post_attention_layernorm.weight', 'base_model.model.layers.16.self_attn.q_proj.base_layer.weight', 'base_model.model.layers.16.self_attn.q_proj.lora_A.default.weight', 'base_model.model.layers.16.self_attn.q_proj.lora_B.default.weight', 'base_model.model.layers.16.self_attn.k_proj.base_layer.weight', 'base_model.model.layers.16.self_attn.k_proj.lora_A.default.weight', 'base_model.model.layers.16.self_attn.k_proj.lora_B.default.weight', 'base_model.model.layers.16.self_attn.v_proj.base_layer.weight', 'base_model.model.layers.16.self_attn.v_proj.lora_A.default.weight', 'base_model.model.layers.16.self_attn.v_proj.lora_B.default.weight', 'base_model.model.layers.16.self_attn.o_proj.base_layer.weight', 'base_model.model.layers.16.self_attn.o_proj.lora_A.default.weight', 'base_model.model.layers.16.self_attn.o_proj.lora_B.default.weight', 'base_model.model.layers.16.mlp.gate_proj.base_layer.weight', 'base_model.model.layers.16.mlp.gate_proj.lora_A.default.weight', 'base_model.model.layers.16.mlp.gate_proj.lora_B.default.weight', 'base_model.model.layers.16.mlp.up_proj.base_layer.weight', 'base_model.model.layers.16.mlp.up_proj.lora_A.default.weight', 'base_model.model.layers.16.mlp.up_proj.lora_B.default.weight', 'base_model.model.layers.16.mlp.down_proj.base_layer.weight', 'base_model.model.layers.16.mlp.down_proj.lora_A.default.weight', 'base_model.model.layers.16.mlp.down_proj.lora_B.default.weight', 'base_model.model.layers.16.input_layernorm.weight', 'base_model.model.layers.16.post_attention_layernorm.weight', 'base_model.model.layers.17.self_attn.q_proj.base_layer.weight', 'base_model.model.layers.17.self_attn.q_proj.lora_A.default.weight', 'base_model.model.layers.17.self_attn.q_proj.lora_B.default.weight', 'base_model.model.layers.17.self_attn.k_proj.base_layer.weight', 'base_model.model.layers.17.self_attn.k_proj.lora_A.default.weight', 'base_model.model.layers.17.self_attn.k_proj.lora_B.default.weight', 'base_model.model.layers.17.self_attn.v_proj.base_layer.weight', 'base_model.model.layers.17.self_attn.v_proj.lora_A.default.weight', 'base_model.model.layers.17.self_attn.v_proj.lora_B.default.weight', 'base_model.model.layers.17.self_attn.o_proj.base_layer.weight', 'base_model.model.layers.17.self_attn.o_proj.lora_A.default.weight', 'base_model.model.layers.17.self_attn.o_proj.lora_B.default.weight', 'base_model.model.layers.17.mlp.gate_proj.base_layer.weight', 'base_model.model.layers.17.mlp.gate_proj.lora_A.default.weight', 'base_model.model.layers.17.mlp.gate_proj.lora_B.default.weight', 'base_model.model.layers.17.mlp.up_proj.base_layer.weight', 'base_model.model.layers.17.mlp.up_proj.lora_A.default.weight', 'base_model.model.layers.17.mlp.up_proj.lora_B.default.weight', 'base_model.model.layers.17.mlp.down_proj.base_layer.weight', 'base_model.model.layers.17.mlp.down_proj.lora_A.default.weight', 'base_model.model.layers.17.mlp.down_proj.lora_B.default.weight', 'base_model.model.layers.17.input_layernorm.weight', 'base_model.model.layers.17.post_attention_layernorm.weight', 'base_model.model.layers.18.self_attn.q_proj.base_layer.weight', 'base_model.model.layers.18.self_attn.q_proj.lora_A.default.weight', 'base_model.model.layers.18.self_attn.q_proj.lora_B.default.weight', 'base_model.model.layers.18.self_attn.k_proj.base_layer.weight', 'base_model.model.layers.18.self_attn.k_proj.lora_A.default.weight', 'base_model.model.layers.18.self_attn.k_proj.lora_B.default.weight', 'base_model.model.layers.18.self_attn.v_proj.base_layer.weight', 'base_model.model.layers.18.self_attn.v_proj.lora_A.default.weight', 'base_model.model.layers.18.self_attn.v_proj.lora_B.default.weight', 'base_model.model.layers.18.self_attn.o_proj.base_layer.weight', 'base_model.model.layers.18.self_attn.o_proj.lora_A.default.weight', 'base_model.model.layers.18.self_attn.o_proj.lora_B.default.weight', 'base_model.model.layers.18.mlp.gate_proj.base_layer.weight', 'base_model.model.layers.18.mlp.gate_proj.lora_A.default.weight', 'base_model.model.layers.18.mlp.gate_proj.lora_B.default.weight', 'base_model.model.layers.18.mlp.up_proj.base_layer.weight', 'base_model.model.layers.18.mlp.up_proj.lora_A.default.weight', 'base_model.model.layers.18.mlp.up_proj.lora_B.default.weight', 'base_model.model.layers.18.mlp.down_proj.base_layer.weight', 'base_model.model.layers.18.mlp.down_proj.lora_A.default.weight', 'base_model.model.layers.18.mlp.down_proj.lora_B.default.weight', 'base_model.model.layers.18.input_layernorm.weight', 'base_model.model.layers.18.post_attention_layernorm.weight', 'base_model.model.layers.19.self_attn.q_proj.base_layer.weight', 'base_model.model.layers.19.self_attn.q_proj.lora_A.default.weight', 'base_model.model.layers.19.self_attn.q_proj.lora_B.default.weight', 'base_model.model.layers.19.self_attn.k_proj.base_layer.weight', 'base_model.model.layers.19.self_attn.k_proj.lora_A.default.weight', 'base_model.model.layers.19.self_attn.k_proj.lora_B.default.weight', 'base_model.model.layers.19.self_attn.v_proj.base_layer.weight', 'base_model.model.layers.19.self_attn.v_proj.lora_A.default.weight', 'base_model.model.layers.19.self_attn.v_proj.lora_B.default.weight', 'base_model.model.layers.19.self_attn.o_proj.base_layer.weight', 'base_model.model.layers.19.self_attn.o_proj.lora_A.default.weight', 'base_model.model.layers.19.self_attn.o_proj.lora_B.default.weight', 'base_model.model.layers.19.mlp.gate_proj.base_layer.weight', 'base_model.model.layers.19.mlp.gate_proj.lora_A.default.weight', 'base_model.model.layers.19.mlp.gate_proj.lora_B.default.weight', 'base_model.model.layers.19.mlp.up_proj.base_layer.weight', 'base_model.model.layers.19.mlp.up_proj.lora_A.default.weight', 'base_model.model.layers.19.mlp.up_proj.lora_B.default.weight', 'base_model.model.layers.19.mlp.down_proj.base_layer.weight', 'base_model.model.layers.19.mlp.down_proj.lora_A.default.weight', 'base_model.model.layers.19.mlp.down_proj.lora_B.default.weight', 'base_model.model.layers.19.input_layernorm.weight', 'base_model.model.layers.19.post_attention_layernorm.weight', 'base_model.model.layers.20.self_attn.q_proj.base_layer.weight', 'base_model.model.layers.20.self_attn.q_proj.lora_A.default.weight', 'base_model.model.layers.20.self_attn.q_proj.lora_B.default.weight', 'base_model.model.layers.20.self_attn.k_proj.base_layer.weight', 'base_model.model.layers.20.self_attn.k_proj.lora_A.default.weight', 'base_model.model.layers.20.self_attn.k_proj.lora_B.default.weight', 'base_model.model.layers.20.self_attn.v_proj.base_layer.weight', 'base_model.model.layers.20.self_attn.v_proj.lora_A.default.weight', 'base_model.model.layers.20.self_attn.v_proj.lora_B.default.weight', 'base_model.model.layers.20.self_attn.o_proj.base_layer.weight', 'base_model.model.layers.20.self_attn.o_proj.lora_A.default.weight', 'base_model.model.layers.20.self_attn.o_proj.lora_B.default.weight', 'base_model.model.layers.20.mlp.gate_proj.base_layer.weight', 'base_model.model.layers.20.mlp.gate_proj.lora_A.default.weight', 'base_model.model.layers.20.mlp.gate_proj.lora_B.default.weight', 'base_model.model.layers.20.mlp.up_proj.base_layer.weight', 'base_model.model.layers.20.mlp.up_proj.lora_A.default.weight', 'base_model.model.layers.20.mlp.up_proj.lora_B.default.weight', 'base_model.model.layers.20.mlp.down_proj.base_layer.weight', 'base_model.model.layers.20.mlp.down_proj.lora_A.default.weight', 'base_model.model.layers.20.mlp.down_proj.lora_B.default.weight', 'base_model.model.layers.20.input_layernorm.weight', 'base_model.model.layers.20.post_attention_layernorm.weight', 'base_model.model.layers.21.self_attn.q_proj.base_layer.weight', 'base_model.model.layers.21.self_attn.q_proj.lora_A.default.weight', 'base_model.model.layers.21.self_attn.q_proj.lora_B.default.weight', 'base_model.model.layers.21.self_attn.k_proj.base_layer.weight', 'base_model.model.layers.21.self_attn.k_proj.lora_A.default.weight', 'base_model.model.layers.21.self_attn.k_proj.lora_B.default.weight', 'base_model.model.layers.21.self_attn.v_proj.base_layer.weight', 'base_model.model.layers.21.self_attn.v_proj.lora_A.default.weight', 'base_model.model.layers.21.self_attn.v_proj.lora_B.default.weight', 'base_model.model.layers.21.self_attn.o_proj.base_layer.weight', 'base_model.model.layers.21.self_attn.o_proj.lora_A.default.weight', 'base_model.model.layers.21.self_attn.o_proj.lora_B.default.weight', 'base_model.model.layers.21.mlp.gate_proj.base_layer.weight', 'base_model.model.layers.21.mlp.gate_proj.lora_A.default.weight', 'base_model.model.layers.21.mlp.gate_proj.lora_B.default.weight', 'base_model.model.layers.21.mlp.up_proj.base_layer.weight', 'base_model.model.layers.21.mlp.up_proj.lora_A.default.weight', 'base_model.model.layers.21.mlp.up_proj.lora_B.default.weight', 'base_model.model.layers.21.mlp.down_proj.base_layer.weight', 'base_model.model.layers.21.mlp.down_proj.lora_A.default.weight', 'base_model.model.layers.21.mlp.down_proj.lora_B.default.weight', 'base_model.model.layers.21.input_layernorm.weight', 'base_model.model.layers.21.post_attention_layernorm.weight', 'base_model.model.layers.22.self_attn.q_proj.base_layer.weight', 'base_model.model.layers.22.self_attn.q_proj.lora_A.default.weight', 'base_model.model.layers.22.self_attn.q_proj.lora_B.default.weight', 'base_model.model.layers.22.self_attn.k_proj.base_layer.weight', 'base_model.model.layers.22.self_attn.k_proj.lora_A.default.weight', 'base_model.model.layers.22.self_attn.k_proj.lora_B.default.weight', 'base_model.model.layers.22.self_attn.v_proj.base_layer.weight', 'base_model.model.layers.22.self_attn.v_proj.lora_A.default.weight', 'base_model.model.layers.22.self_attn.v_proj.lora_B.default.weight', 'base_model.model.layers.22.self_attn.o_proj.base_layer.weight', 'base_model.model.layers.22.self_attn.o_proj.lora_A.default.weight', 'base_model.model.layers.22.self_attn.o_proj.lora_B.default.weight', 'base_model.model.layers.22.mlp.gate_proj.base_layer.weight', 'base_model.model.layers.22.mlp.gate_proj.lora_A.default.weight', 'base_model.model.layers.22.mlp.gate_proj.lora_B.default.weight', 'base_model.model.layers.22.mlp.up_proj.base_layer.weight', 'base_model.model.layers.22.mlp.up_proj.lora_A.default.weight', 'base_model.model.layers.22.mlp.up_proj.lora_B.default.weight', 'base_model.model.layers.22.mlp.down_proj.base_layer.weight', 'base_model.model.layers.22.mlp.down_proj.lora_A.default.weight', 'base_model.model.layers.22.mlp.down_proj.lora_B.default.weight', 'base_model.model.layers.22.input_layernorm.weight', 'base_model.model.layers.22.post_attention_layernorm.weight', 'base_model.model.layers.23.self_attn.q_proj.base_layer.weight', 'base_model.model.layers.23.self_attn.q_proj.lora_A.default.weight', 'base_model.model.layers.23.self_attn.q_proj.lora_B.default.weight', 'base_model.model.layers.23.self_attn.k_proj.base_layer.weight', 'base_model.model.layers.23.self_attn.k_proj.lora_A.default.weight', 'base_model.model.layers.23.self_attn.k_proj.lora_B.default.weight', 'base_model.model.layers.23.self_attn.v_proj.base_layer.weight', 'base_model.model.layers.23.self_attn.v_proj.lora_A.default.weight', 'base_model.model.layers.23.self_attn.v_proj.lora_B.default.weight', 'base_model.model.layers.23.self_attn.o_proj.base_layer.weight', 'base_model.model.layers.23.self_attn.o_proj.lora_A.default.weight', 'base_model.model.layers.23.self_attn.o_proj.lora_B.default.weight', 'base_model.model.layers.23.mlp.gate_proj.base_layer.weight', 'base_model.model.layers.23.mlp.gate_proj.lora_A.default.weight', 'base_model.model.layers.23.mlp.gate_proj.lora_B.default.weight', 'base_model.model.layers.23.mlp.up_proj.base_layer.weight', 'base_model.model.layers.23.mlp.up_proj.lora_A.default.weight', 'base_model.model.layers.23.mlp.up_proj.lora_B.default.weight', 'base_model.model.layers.23.mlp.down_proj.base_layer.weight', 'base_model.model.layers.23.mlp.down_proj.lora_A.default.weight', 'base_model.model.layers.23.mlp.down_proj.lora_B.default.weight', 'base_model.model.layers.23.input_layernorm.weight', 'base_model.model.layers.23.post_attention_layernorm.weight', 'base_model.model.layers.24.self_attn.q_proj.base_layer.weight', 'base_model.model.layers.24.self_attn.q_proj.lora_A.default.weight', 'base_model.model.layers.24.self_attn.q_proj.lora_B.default.weight', 'base_model.model.layers.24.self_attn.k_proj.base_layer.weight', 'base_model.model.layers.24.self_attn.k_proj.lora_A.default.weight', 'base_model.model.layers.24.self_attn.k_proj.lora_B.default.weight', 'base_model.model.layers.24.self_attn.v_proj.base_layer.weight', 'base_model.model.layers.24.self_attn.v_proj.lora_A.default.weight', 'base_model.model.layers.24.self_attn.v_proj.lora_B.default.weight', 'base_model.model.layers.24.self_attn.o_proj.base_layer.weight', 'base_model.model.layers.24.self_attn.o_proj.lora_A.default.weight', 'base_model.model.layers.24.self_attn.o_proj.lora_B.default.weight', 'base_model.model.layers.24.mlp.gate_proj.base_layer.weight', 'base_model.model.layers.24.mlp.gate_proj.lora_A.default.weight', 'base_model.model.layers.24.mlp.gate_proj.lora_B.default.weight', 'base_model.model.layers.24.mlp.up_proj.base_layer.weight', 'base_model.model.layers.24.mlp.up_proj.lora_A.default.weight', 'base_model.model.layers.24.mlp.up_proj.lora_B.default.weight', 'base_model.model.layers.24.mlp.down_proj.base_layer.weight', 'base_model.model.layers.24.mlp.down_proj.lora_A.default.weight', 'base_model.model.layers.24.mlp.down_proj.lora_B.default.weight', 'base_model.model.layers.24.input_layernorm.weight', 'base_model.model.layers.24.post_attention_layernorm.weight', 'base_model.model.layers.25.self_attn.q_proj.base_layer.weight', 'base_model.model.layers.25.self_attn.q_proj.lora_A.default.weight', 'base_model.model.layers.25.self_attn.q_proj.lora_B.default.weight', 'base_model.model.layers.25.self_attn.k_proj.base_layer.weight', 'base_model.model.layers.25.self_attn.k_proj.lora_A.default.weight', 'base_model.model.layers.25.self_attn.k_proj.lora_B.default.weight', 'base_model.model.layers.25.self_attn.v_proj.base_layer.weight', 'base_model.model.layers.25.self_attn.v_proj.lora_A.default.weight', 'base_model.model.layers.25.self_attn.v_proj.lora_B.default.weight', 'base_model.model.layers.25.self_attn.o_proj.base_layer.weight', 'base_model.model.layers.25.self_attn.o_proj.lora_A.default.weight', 'base_model.model.layers.25.self_attn.o_proj.lora_B.default.weight', 'base_model.model.layers.25.mlp.gate_proj.base_layer.weight', 'base_model.model.layers.25.mlp.gate_proj.lora_A.default.weight', 'base_model.model.layers.25.mlp.gate_proj.lora_B.default.weight', 'base_model.model.layers.25.mlp.up_proj.base_layer.weight', 'base_model.model.layers.25.mlp.up_proj.lora_A.default.weight', 'base_model.model.layers.25.mlp.up_proj.lora_B.default.weight', 'base_model.model.layers.25.mlp.down_proj.base_layer.weight', 'base_model.model.layers.25.mlp.down_proj.lora_A.default.weight', 'base_model.model.layers.25.mlp.down_proj.lora_B.default.weight', 'base_model.model.layers.25.input_layernorm.weight', 'base_model.model.layers.25.post_attention_layernorm.weight', 'base_model.model.layers.26.self_attn.q_proj.base_layer.weight', 'base_model.model.layers.26.self_attn.q_proj.lora_A.default.weight', 'base_model.model.layers.26.self_attn.q_proj.lora_B.default.weight', 'base_model.model.layers.26.self_attn.k_proj.base_layer.weight', 'base_model.model.layers.26.self_attn.k_proj.lora_A.default.weight', 'base_model.model.layers.26.self_attn.k_proj.lora_B.default.weight', 'base_model.model.layers.26.self_attn.v_proj.base_layer.weight', 'base_model.model.layers.26.self_attn.v_proj.lora_A.default.weight', 'base_model.model.layers.26.self_attn.v_proj.lora_B.default.weight', 'base_model.model.layers.26.self_attn.o_proj.base_layer.weight', 'base_model.model.layers.26.self_attn.o_proj.lora_A.default.weight', 'base_model.model.layers.26.self_attn.o_proj.lora_B.default.weight', 'base_model.model.layers.26.mlp.gate_proj.base_layer.weight', 'base_model.model.layers.26.mlp.gate_proj.lora_A.default.weight', 'base_model.model.layers.26.mlp.gate_proj.lora_B.default.weight', 'base_model.model.layers.26.mlp.up_proj.base_layer.weight', 'base_model.model.layers.26.mlp.up_proj.lora_A.default.weight', 'base_model.model.layers.26.mlp.up_proj.lora_B.default.weight', 'base_model.model.layers.26.mlp.down_proj.base_layer.weight', 'base_model.model.layers.26.mlp.down_proj.lora_A.default.weight', 'base_model.model.layers.26.mlp.down_proj.lora_B.default.weight', 'base_model.model.layers.26.input_layernorm.weight', 'base_model.model.layers.26.post_attention_layernorm.weight', 'base_model.model.layers.27.self_attn.q_proj.base_layer.weight', 'base_model.model.layers.27.self_attn.q_proj.lora_A.default.weight', 'base_model.model.layers.27.self_attn.q_proj.lora_B.default.weight', 'base_model.model.layers.27.self_attn.k_proj.base_layer.weight', 'base_model.model.layers.27.self_attn.k_proj.lora_A.default.weight', 'base_model.model.layers.27.self_attn.k_proj.lora_B.default.weight', 'base_model.model.layers.27.self_attn.v_proj.base_layer.weight', 'base_model.model.layers.27.self_attn.v_proj.lora_A.default.weight', 'base_model.model.layers.27.self_attn.v_proj.lora_B.default.weight', 'base_model.model.layers.27.self_attn.o_proj.base_layer.weight', 'base_model.model.layers.27.self_attn.o_proj.lora_A.default.weight', 'base_model.model.layers.27.self_attn.o_proj.lora_B.default.weight', 'base_model.model.layers.27.mlp.gate_proj.base_layer.weight', 'base_model.model.layers.27.mlp.gate_proj.lora_A.default.weight', 'base_model.model.layers.27.mlp.gate_proj.lora_B.default.weight', 'base_model.model.layers.27.mlp.up_proj.base_layer.weight', 'base_model.model.layers.27.mlp.up_proj.lora_A.default.weight', 'base_model.model.layers.27.mlp.up_proj.lora_B.default.weight', 'base_model.model.layers.27.mlp.down_proj.base_layer.weight', 'base_model.model.layers.27.mlp.down_proj.lora_A.default.weight', 'base_model.model.layers.27.mlp.down_proj.lora_B.default.weight', 'base_model.model.layers.27.input_layernorm.weight', 'base_model.model.layers.27.post_attention_layernorm.weight', 'base_model.model.layers.28.self_attn.q_proj.base_layer.weight', 'base_model.model.layers.28.self_attn.q_proj.lora_A.default.weight', 'base_model.model.layers.28.self_attn.q_proj.lora_B.default.weight', 'base_model.model.layers.28.self_attn.k_proj.base_layer.weight', 'base_model.model.layers.28.self_attn.k_proj.lora_A.default.weight', 'base_model.model.layers.28.self_attn.k_proj.lora_B.default.weight', 'base_model.model.layers.28.self_attn.v_proj.base_layer.weight', 'base_model.model.layers.28.self_attn.v_proj.lora_A.default.weight', 'base_model.model.layers.28.self_attn.v_proj.lora_B.default.weight', 'base_model.model.layers.28.self_attn.o_proj.base_layer.weight', 'base_model.model.layers.28.self_attn.o_proj.lora_A.default.weight', 'base_model.model.layers.28.self_attn.o_proj.lora_B.default.weight', 'base_model.model.layers.28.mlp.gate_proj.base_layer.weight', 'base_model.model.layers.28.mlp.gate_proj.lora_A.default.weight', 'base_model.model.layers.28.mlp.gate_proj.lora_B.default.weight', 'base_model.model.layers.28.mlp.up_proj.base_layer.weight', 'base_model.model.layers.28.mlp.up_proj.lora_A.default.weight', 'base_model.model.layers.28.mlp.up_proj.lora_B.default.weight', 'base_model.model.layers.28.mlp.down_proj.base_layer.weight', 'base_model.model.layers.28.mlp.down_proj.lora_A.default.weight', 'base_model.model.layers.28.mlp.down_proj.lora_B.default.weight', 'base_model.model.layers.28.input_layernorm.weight', 'base_model.model.layers.28.post_attention_layernorm.weight', 'base_model.model.layers.29.self_attn.q_proj.base_layer.weight', 'base_model.model.layers.29.self_attn.q_proj.lora_A.default.weight', 'base_model.model.layers.29.self_attn.q_proj.lora_B.default.weight', 'base_model.model.layers.29.self_attn.k_proj.base_layer.weight', 'base_model.model.layers.29.self_attn.k_proj.lora_A.default.weight', 'base_model.model.layers.29.self_attn.k_proj.lora_B.default.weight', 'base_model.model.layers.29.self_attn.v_proj.base_layer.weight', 'base_model.model.layers.29.self_attn.v_proj.lora_A.default.weight', 'base_model.model.layers.29.self_attn.v_proj.lora_B.default.weight', 'base_model.model.layers.29.self_attn.o_proj.base_layer.weight', 'base_model.model.layers.29.self_attn.o_proj.lora_A.default.weight', 'base_model.model.layers.29.self_attn.o_proj.lora_B.default.weight', 'base_model.model.layers.29.mlp.gate_proj.base_layer.weight', 'base_model.model.layers.29.mlp.gate_proj.lora_A.default.weight', 'base_model.model.layers.29.mlp.gate_proj.lora_B.default.weight', 'base_model.model.layers.29.mlp.up_proj.base_layer.weight', 'base_model.model.layers.29.mlp.up_proj.lora_A.default.weight', 'base_model.model.layers.29.mlp.up_proj.lora_B.default.weight', 'base_model.model.layers.29.mlp.down_proj.base_layer.weight', 'base_model.model.layers.29.mlp.down_proj.lora_A.default.weight', 'base_model.model.layers.29.mlp.down_proj.lora_B.default.weight', 'base_model.model.layers.29.input_layernorm.weight', 'base_model.model.layers.29.post_attention_layernorm.weight', 'base_model.model.layers.30.self_attn.q_proj.base_layer.weight', 'base_model.model.layers.30.self_attn.q_proj.lora_A.default.weight', 'base_model.model.layers.30.self_attn.q_proj.lora_B.default.weight', 'base_model.model.layers.30.self_attn.k_proj.base_layer.weight', 'base_model.model.layers.30.self_attn.k_proj.lora_A.default.weight', 'base_model.model.layers.30.self_attn.k_proj.lora_B.default.weight', 'base_model.model.layers.30.self_attn.v_proj.base_layer.weight', 'base_model.model.layers.30.self_attn.v_proj.lora_A.default.weight', 'base_model.model.layers.30.self_attn.v_proj.lora_B.default.weight', 'base_model.model.layers.30.self_attn.o_proj.base_layer.weight', 'base_model.model.layers.30.self_attn.o_proj.lora_A.default.weight', 'base_model.model.layers.30.self_attn.o_proj.lora_B.default.weight', 'base_model.model.layers.30.mlp.gate_proj.base_layer.weight', 'base_model.model.layers.30.mlp.gate_proj.lora_A.default.weight', 'base_model.model.layers.30.mlp.gate_proj.lora_B.default.weight', 'base_model.model.layers.30.mlp.up_proj.base_layer.weight', 'base_model.model.layers.30.mlp.up_proj.lora_A.default.weight', 'base_model.model.layers.30.mlp.up_proj.lora_B.default.weight', 'base_model.model.layers.30.mlp.down_proj.base_layer.weight', 'base_model.model.layers.30.mlp.down_proj.lora_A.default.weight', 'base_model.model.layers.30.mlp.down_proj.lora_B.default.weight', 'base_model.model.layers.30.input_layernorm.weight', 'base_model.model.layers.30.post_attention_layernorm.weight', 'base_model.model.layers.31.self_attn.q_proj.base_layer.weight', 'base_model.model.layers.31.self_attn.q_proj.lora_A.default.weight', 'base_model.model.layers.31.self_attn.q_proj.lora_B.default.weight', 'base_model.model.layers.31.self_attn.k_proj.base_layer.weight', 'base_model.model.layers.31.self_attn.k_proj.lora_A.default.weight', 'base_model.model.layers.31.self_attn.k_proj.lora_B.default.weight', 'base_model.model.layers.31.self_attn.v_proj.base_layer.weight', 'base_model.model.layers.31.self_attn.v_proj.lora_A.default.weight', 'base_model.model.layers.31.self_attn.v_proj.lora_B.default.weight', 'base_model.model.layers.31.self_attn.o_proj.base_layer.weight', 'base_model.model.layers.31.self_attn.o_proj.lora_A.default.weight', 'base_model.model.layers.31.self_attn.o_proj.lora_B.default.weight', 'base_model.model.layers.31.mlp.gate_proj.base_layer.weight', 'base_model.model.layers.31.mlp.gate_proj.lora_A.default.weight', 'base_model.model.layers.31.mlp.gate_proj.lora_B.default.weight', 'base_model.model.layers.31.mlp.up_proj.base_layer.weight', 'base_model.model.layers.31.mlp.up_proj.lora_A.default.weight', 'base_model.model.layers.31.mlp.up_proj.lora_B.default.weight', 'base_model.model.layers.31.mlp.down_proj.base_layer.weight', 'base_model.model.layers.31.mlp.down_proj.lora_A.default.weight', 'base_model.model.layers.31.mlp.down_proj.lora_B.default.weight', 'base_model.model.layers.31.input_layernorm.weight', 'base_model.model.layers.31.post_attention_layernorm.weight', 'base_model.model.norm.weight'], unexpected_keys=[])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.backbone.model.load_adapter(\"./outputs/early_stop/test\", adapter_name=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/anhnh/miniconda3/envs/commonenv/lib/python3.10/site-packages/peft/utils/other.py:581: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-66dc0f55-7240e45859f6a4084e12c9e5;6d238209-4a39-44f6-baaa-75dc898cbda2)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct/resolve/main/config.json.\n",
      "Access to model meta-llama/Meta-Llama-3-8B-Instruct is restricted. You must be authenticated to access it. - silently ignoring the lookup for the file config.json in meta-llama/Meta-Llama-3-8B-Instruct.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model.backbone.model.save_pretrained(\"./outputs/early_stop/test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4923973083496094"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Start the timer\n",
    "start_time = time.time()\n",
    "\n",
    "# Operation\n",
    "model.to('cuda:1')\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# End the timer\n",
    "time.time() - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CUDA_LAUNCH_BLOCKING=1 python train.py \\\n",
    "                            --data-root ./data_incremental \\\n",
    "                            --dataset ACE \\\n",
    "                            --backbone llm2vec \\\n",
    "                            --lr 2e-5 \\\n",
    "                            --decay 1e-4 \\\n",
    "                            --no-freeze-bert \\\n",
    "                            --shot-num 1 \\\n",
    "                            --batch-size 2 \\\n",
    "                            --device cuda:0 \\\n",
    "                            --log \\\n",
    "                            --log-dir ./outputs/log_incremental/temp7_submax/first_wo_UCL+TCL/ \\\n",
    "                            --log-name a0_lnone_r5 \\\n",
    "                            --dweight_loss \\\n",
    "                            --rep-aug mean \\\n",
    "                            --distill mul \\\n",
    "                            --epochs 20 \\\n",
    "                            --class-num 10 \\\n",
    "                            --single-label \\\n",
    "                            --cl-aug shuffle \\\n",
    "                            --aug-repeat-times 5 \\\n",
    "                            --joint-da-loss none \\\n",
    "                            --sub-max \\\n",
    "                            --cl_temp 0.07 \\\n",
    "                            --tlcl \\\n",
    "                            --ucl \\\n",
    "                            --skip-first-cl ucl+tlcl \\\n",
    "                            --perm-id 1\n",
    "                            --my-test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "do\n",
    "    for j in 5 10\n",
    "    do\n",
    "        for k in shuffle\n",
    "        do\n",
    "            for m in 10 20\n",
    "            do\n",
    "                for n in ACE MAVEN\n",
    "                do\n",
    "                    python train.py \\\n",
    "                        --data-root ./data_incremental \\\n",
    "                        --dataset $n \\\n",
    "                        --backbone bert-base-uncased \\\n",
    "                        --lr 2e-5 \\\n",
    "                        --decay 1e-4 \\\n",
    "                        --no-freeze-bert \\\n",
    "                        --shot-num $j \\\n",
    "                        --batch-size 4 \\\n",
    "                        --device cuda:4 \\\n",
    "                        --log \\\n",
    "                        --log-dir ./outputs/log_incremental/temp7_submax/first_wo_UCL+TCL/ \\\n",
    "                        --log-name a${k}_l${l}_r${i} \\\n",
    "                        --dweight_loss \\\n",
    "                        --rep-aug mean \\\n",
    "                        --distill mul \\\n",
    "                        --epoch 30 \\\n",
    "                        --class-num $m \\\n",
    "                        --single-label \\\n",
    "                        --cl-aug $k \\\n",
    "                        --aug-repeat-times $i \\\n",
    "                        --joint-da-loss $l \\\n",
    "                        --sub-max \\\n",
    "                        --cl_temp 0.07 \\\n",
    "                        --tlcl \\\n",
    "                        --ucl \\\n",
    "                        --skip-first-cl ucl+tlcl\n",
    "                done\n",
    "            done\n",
    "        done\n",
    "    done\n",
    "done\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([260])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "train_y = [torch.tensor([3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "        0, 0, 0], device='cuda:0'), torch.tensor([1], device='cuda:0')]\n",
    "da_y = [torch.tensor([3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "        0, 0, 0], device='cuda:0'), torch.tensor([1], device='cuda:0'), torch.tensor([3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "        0, 0, 0], device='cuda:0'), torch.tensor([1], device='cuda:0'), torch.tensor([3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "        0, 0, 0], device='cuda:0'), torch.tensor([1], device='cuda:0'), torch.tensor([3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "        0, 0, 0], device='cuda:0'), torch.tensor([1], device='cuda:0'), torch.tensor([3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "        0, 0, 0], device='cuda:0'), torch.tensor([1], device='cuda:0')]\n",
    "\n",
    "torch.cat(da_y).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0, 0]), tensor([0, 0]), tensor([0, 1]))"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.where(\n",
    "    torch.tensor([[[1,2,2]]]) == torch.tensor([[[1,2,3]]])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([49, 2])"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "old_span = torch.tensor([[115, 115],\n",
    "        [ 85,  85],\n",
    "        [ 86,  86],\n",
    "        [ 87,  87],\n",
    "        [ 88,  88],\n",
    "        [ 89,  89],\n",
    "        [ 90,  90],\n",
    "        [ 91,  91],\n",
    "        [ 92,  92],\n",
    "        [ 93,  93],\n",
    "        [ 94,  94],\n",
    "        [ 95,  95],\n",
    "        [ 96,  96],\n",
    "        [ 97,  97],\n",
    "        [ 98,  98],\n",
    "        [ 99,  99],\n",
    "        [100, 100],\n",
    "        [101, 101],\n",
    "        [102, 102],\n",
    "        [103, 103],\n",
    "        [104, 104],\n",
    "        [105, 105],\n",
    "        [106, 106],\n",
    "        [107, 107],\n",
    "        [108, 108],\n",
    "        [109, 109],\n",
    "        [110, 110],\n",
    "        [111, 111],\n",
    "        [112, 112],\n",
    "        [113, 113],\n",
    "        [114, 114],\n",
    "        [116, 116],\n",
    "        [117, 117],\n",
    "        [118, 118],\n",
    "        [119, 119],\n",
    "        [120, 120],\n",
    "        [121, 121],\n",
    "        [122, 122],\n",
    "        [123, 123],\n",
    "        [124, 124],\n",
    "        [125, 125],\n",
    "        [126, 126],\n",
    "        [127, 127],\n",
    "        [128, 128],\n",
    "        [129, 129],\n",
    "        [130, 130],\n",
    "        [131, 131],\n",
    "        [134, 134],\n",
    "        [135, 135]])\n",
    "\n",
    "old_span.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([133, 133])\n",
      "tensor([132, 132])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([49, 2])"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = torch.randperm(51).unsqueeze(0).unsqueeze(0) + 135 - 51 + 1\n",
    "new_span = torch.where(old_span.unsqueeze(2).cpu() == z.cpu())[2].view(-1, 2) + 135 - 51 + 1\n",
    "\n",
    "for span in new_span:\n",
    "    if span not in old_span:\n",
    "        print(span)\n",
    "\n",
    "new_span.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[100, 116, 126,  84, 106,  94,  95, 123, 104, 105,  97, 131, 108,  90,\n",
       "          129, 113, 125, 111, 132, 117, 124,  92, 128, 135,  85,  91,  88, 101,\n",
       "          134, 114, 109, 102, 130,  93,  99,  98, 103, 118, 122,  87,  96,  89,\n",
       "          127, 110, 133, 119, 115, 112, 107,  86, 121, 120]]])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.sort(\n",
       "values=tensor([ 85,  85,  86,  86,  87,  87,  88,  88,  89,  89,  90,  90,  91,  91,\n",
       "         92,  92,  93,  93,  94,  94,  95,  95,  96,  96,  97,  97,  98,  98,\n",
       "         99,  99, 100, 100, 101, 101, 102, 102, 103, 103, 104, 104, 105, 105,\n",
       "        106, 106, 107, 107, 108, 108, 109, 109, 110, 110, 111, 111, 112, 112,\n",
       "        113, 113, 114, 114, 115, 115, 116, 116, 117, 117, 118, 118, 119, 119,\n",
       "        120, 120, 121, 121, 122, 122, 124, 124, 125, 125, 127, 127, 128, 128,\n",
       "        129, 129, 130, 130, 131, 131, 132, 132, 133, 133, 134, 134, 135, 135]),\n",
       "indices=tensor([90, 91, 77, 76, 72, 73, 89, 88, 36, 37, 16, 17, 61, 60, 85, 84, 18, 19,\n",
       "        33, 32, 55, 54, 70, 71, 28, 29, 65, 64, 42, 43, 57, 56, 68, 69, 62, 63,\n",
       "        80, 81, 93, 92, 27, 26, 35, 34, 97, 96, 95, 94, 66, 67, 87, 86, 75, 74,\n",
       "        52, 53, 49, 48,  2,  3, 11, 10, 51, 50,  1,  0,  9,  8, 78, 79, 12, 13,\n",
       "        39, 38, 45, 44, 14, 15, 20, 21, 58, 59,  7,  6, 47, 46, 30, 31, 82, 83,\n",
       "        24, 25, 23, 22, 41, 40,  5,  4]))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_span.view(-1).sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([51, 2])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "old_span.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50, 2, 1])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "da_span[0].unsqueeze(2).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([98])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11, 602, 1781, 430, 279, 2944, 16470, 304, 279, 10007, 482, 482, 499, 1440, 11, 1176, 315, 682, 11, 584, 1051, 482, 482, 994, 26875, 3817, 938, 974, 43521, 574, 16689, 4872, 11, 584, 1047, 1027, 5496, 1148, 584, 3463, 574, 2103, 264, 69912, 7140, 1306, 279, 8431, 4208, 13, 128001]\n"
     ]
    }
   ],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0, 3, 2, 1, 4]]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
